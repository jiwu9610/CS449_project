{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":674,"status":"ok","timestamp":1686267377352,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"oEPk6qre9y3R","outputId":"333b84ff-5a64-41af-c1b5-c1fd4e024fd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'lex-glue'...\n","remote: Enumerating objects: 291, done.\u001b[K\n","remote: Counting objects: 100% (139/139), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 291 (delta 109), reused 96 (delta 76), pack-reused 152\u001b[K\n","Receiving objects: 100% (291/291), 101.14 KiB | 20.23 MiB/s, done.\n","Resolving deltas: 100% (178/178), done.\n"]}],"source":["!git clone https://github.com/BlueSocksFFF/lex-glue.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686267377352,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"-uERvyxGOWam","outputId":"f6706fcc-15fc-4d40-e455-657a049ca359"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/lex-glue\n"]}],"source":["cd lex-glue"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20225,"status":"ok","timestamp":1686267397576,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"Hh9gbRV7OTfq","outputId":"1ac24f2f-7af8-4346-d4bb-bd19dd06e438"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n","Collecting transformers>=4.9.0 (from -r requirements.txt (line 2))\n","  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.2.2)\n","Requirement already satisfied: tqdm>=4.61.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.65.0)\n","Requirement already satisfied: numpy>=1.20.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.22.4)\n","Collecting datasets>=1.18.1 (from -r requirements.txt (line 6))\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.1)\n","Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->-r requirements.txt (line 1)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->-r requirements.txt (line 1)) (16.0.5)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.9.0->-r requirements.txt (line 2))\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.0->-r requirements.txt (line 2)) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.0->-r requirements.txt (line 2)) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.0->-r requirements.txt (line 2)) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.0->-r requirements.txt (line 2)) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.9.0->-r requirements.txt (line 2))\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.9.0->-r requirements.txt (line 2))\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->-r requirements.txt (line 3)) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->-r requirements.txt (line 3)) (3.1.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.1->-r requirements.txt (line 6)) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.1->-r requirements.txt (line 6)) (1.5.3)\n","Collecting xxhash (from datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.18.1->-r requirements.txt (line 6)) (2023.4.0)\n","Collecting aiohttp (from datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19 (from datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->-r requirements.txt (line 7)) (8.1.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6)) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6)) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=1.18.1->-r requirements.txt (line 6))\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.9.0->-r requirements.txt (line 2)) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.9.0->-r requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.9.0->-r requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->-r requirements.txt (line 1)) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.18.1->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.18.1->-r requirements.txt (line 6)) (2022.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.18.1->-r requirements.txt (line 6)) (1.16.0)\n","Installing collected packages: tokenizers, safetensors, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.0 xxhash-3.2.0 yarl-1.9.2\n"]}],"source":["%pip install -r requirements.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5874,"status":"ok","timestamp":1686267403449,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"8Uq5onkv3zib","outputId":"eeeef33b-5639-466e-d8f4-2892464f849b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.20.3\n"]}],"source":["%pip install --upgrade accelerate"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686267403449,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"na5a0HTCgnY2"},"outputs":[],"source":["model_folder_path = '/content/gdrive/MyDrive/saved_model/'\n","\n","model_path = 'bert_full_more_data'\n","\n","script_file = 'scripts/run_case_hold.sh'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17866,"status":"ok","timestamp":1686267421313,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"oYvbbV_qChr5","outputId":"5efb82b7-0e95-4248-9bf6-adeb46588fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Specify the file path\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686267421314,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"eLFProg9rSMk"},"outputs":[],"source":["# Read in the file\n","with open(script_file, 'r') as file :\n","  filedata = file.read()\n","\n","# Replace the target string\n","filedata = filedata.replace('\\'pretrained_models/bert_mask_10.pth\\'', '\\''+model_folder_path+model_path+'\\'')\n","# filedata = filedata.replace('\\'bert-base-uncased\\'', '\\''+'bert-base-uncased'+'\\'')\n","\n","# Write the file out again\n","with open(script_file, 'w') as file:\n","  file.write(filedata)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gc2VREoaN4J","executionInfo":{"status":"ok","timestamp":1686270984717,"user_tz":300,"elapsed":3563405,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"}},"outputId":"6ce31446-8ffc-40cb-977a-c5c2f4ff4658"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-08 23:37:06.289528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","06/08/2023 23:37:08 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n","06/08/2023 23:37:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=1,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=True,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=True,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/runs/Jun08_23-37-08_a120e79ba634,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=micro-f1,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=20.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=5,\n","seed=1,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.70MB/s]\n","[INFO|configuration_utils.py:669] 2023-06-08 23:37:09,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/config.json\n","[INFO|configuration_utils.py:725] 2023-06-08 23:37:09,143 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.30.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 236kB/s]\n","[INFO|configuration_utils.py:669] 2023-06-08 23:37:09,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/config.json\n","[INFO|configuration_utils.py:725] 2023-06-08 23:37:09,590 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.30.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 49.8MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 732kB/s]\n","[INFO|tokenization_utils_base.py:1823] 2023-06-08 23:37:11,623 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/vocab.txt\n","[INFO|tokenization_utils_base.py:1823] 2023-06-08 23:37:11,623 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/tokenizer.json\n","[INFO|tokenization_utils_base.py:1823] 2023-06-08 23:37:11,623 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1823] 2023-06-08 23:37:11,623 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1823] 2023-06-08 23:37:11,623 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/tokenizer_config.json\n","[INFO|configuration_utils.py:669] 2023-06-08 23:37:11,623 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/config.json\n","[INFO|configuration_utils.py:725] 2023-06-08 23:37:11,624 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.30.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|modeling_utils.py:2575] 2023-06-08 23:37:12,040 >> loading weights file /content/gdrive/MyDrive/saved_model/bert_full_more_data/pytorch_model.bin\n","[WARNING|modeling_utils.py:3285] 2023-06-08 23:37:16,890 >> Some weights of the model checkpoint at /content/gdrive/MyDrive/saved_model/bert_full_more_data were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3297] 2023-06-08 23:37:16,890 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at /content/gdrive/MyDrive/saved_model/bert_full_more_data and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Downloading builder script: 100% 23.3k/23.3k [00:00<00:00, 20.2MB/s]\n","Downloading metadata: 100% 21.0k/21.0k [00:00<00:00, 16.1MB/s]\n","Downloading readme: 100% 32.9k/32.9k [00:00<00:00, 24.1MB/s]\n","Downloading and preparing dataset lex_glue/case_hold to /root/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a...\n","Downloading data: 100% 30.4M/30.4M [00:03<00:00, 7.67MB/s]\n","Dataset lex_glue downloaded and prepared to /root/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 398.56it/s]\n","06/08/2023 23:37:36 - INFO - casehold_helpers -   Creating features from dataset file at case_hold\n","06/08/2023 23:37:36 - INFO - casehold_helpers -   Training examples: 45000\n","convert examples to features: 0it [00:00, ?it/s]06/08/2023 23:37:36 - INFO - casehold_helpers -   Writing example 0 of 45000\n","convert examples to features: 9994it [00:43, 218.75it/s]06/08/2023 23:38:20 - INFO - casehold_helpers -   Writing example 10000 of 45000\n","convert examples to features: 19982it [01:29, 211.83it/s]06/08/2023 23:39:06 - INFO - casehold_helpers -   Writing example 20000 of 45000\n","convert examples to features: 29993it [02:17, 205.27it/s]06/08/2023 23:39:54 - INFO - casehold_helpers -   Writing example 30000 of 45000\n","convert examples to features: 39981it [03:06, 208.92it/s]06/08/2023 23:40:43 - INFO - casehold_helpers -   Writing example 40000 of 45000\n","convert examples to features: 45000it [03:31, 212.96it/s]\n","06/08/2023 23:41:07 - INFO - casehold_helpers -   *** Example ***\n","06/08/2023 23:41:07 - INFO - casehold_helpers -   feature: InputFeatures(input_ids=[[101, 2852, 24065, 4887, 1521, 1055, 2522, 27794, 2015, 1010, 1996, 2522, 27794, 2052, 2022, 1037, 1523, 6778, 1524, 1997, 2437, 1996, 5968, 1012, 2582, 1010, 2543, 5092, 29232, 2024, 26096, 4795, 1012, 2045, 2003, 2053, 9379, 3800, 2005, 2437, 1037, 5968, 1012, 24648, 25173, 2008, 9125, 14792, 7515, 2004, 1523, 6355, 6997, 1524, 2005, 5682, 1997, 20226, 1996, 11746, 1997, 2476, 19591, 1012, 2156, 2324, 1057, 1012, 1055, 1012, 1039, 1012, 1073, 6227, 2549, 1006, 1041, 1007, 1006, 1016, 1007, 1006, 1038, 1007, 1006, 2462, 1007, 1006, 12854, 1037, 1523, 6355, 24648, 1524, 2004, 1024, 1523, 2151, 4126, 16385, 3085, 2011, 10219, 2005, 1037, 2744, 17003, 2028, 2095, 1012, 1012, 1012, 2008, 1012, 1012, 1012, 7336, 2224, 1997, 14792, 1524, 1007, 1012, 5434, 2031, 2179, 6664, 1997, 1037, 1005, 5968, 2000, 2022, 1037, 4126, 1997, 4808, 2241, 2006, 1996, 3768, 1997, 1037, 2512, 25500, 16136, 3800, 2005, 1037, 5968, 1998, 1996, 2755, 2008, 1010, 2011, 2049, 2200, 3267, 1010, 2045, 2003, 1037, 6937, 3891, 2008, 1996, 5968, 2052, 2022, 2109, 2114, 1996, 2711, 2030, 3200, 1997, 2178, 1012, 2156, 2142, 2163, 1058, 1012, 10625, 1010, 8732, 1042, 1012, 7605, 6564, 2509, 1006, 6049, 25022, 2099, 1012, 2722, 1007, 1006, 19106, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 11898, 1010, 6391, 2575, 1042, 1012, 10514, 9397, 1012, 18596, 1010, 102, 3173, 2008, 6664, 1997, 1037, 8667, 5968, 2003, 1037, 4126, 1997, 4808, 2005, 5682, 1997, 2324, 15529, 26257, 2475, 2546, 2487, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2852, 24065, 4887, 1521, 1055, 2522, 27794, 2015, 1010, 1996, 2522, 27794, 2052, 2022, 1037, 1523, 6778, 1524, 1997, 2437, 1996, 5968, 1012, 2582, 1010, 2543, 5092, 29232, 2024, 26096, 4795, 1012, 2045, 2003, 2053, 9379, 3800, 2005, 2437, 1037, 5968, 1012, 24648, 25173, 2008, 9125, 14792, 7515, 2004, 1523, 6355, 6997, 1524, 2005, 5682, 1997, 20226, 1996, 11746, 1997, 2476, 19591, 1012, 2156, 2324, 1057, 1012, 1055, 1012, 1039, 1012, 1073, 6227, 2549, 1006, 1041, 1007, 1006, 1016, 1007, 1006, 1038, 1007, 1006, 2462, 1007, 1006, 12854, 1037, 1523, 6355, 24648, 1524, 2004, 1024, 1523, 2151, 4126, 16385, 3085, 2011, 10219, 2005, 1037, 2744, 17003, 2028, 2095, 1012, 1012, 1012, 2008, 1012, 1012, 1012, 7336, 2224, 1997, 14792, 1524, 1007, 1012, 5434, 2031, 2179, 6664, 1997, 1037, 1005, 5968, 2000, 2022, 1037, 4126, 1997, 4808, 2241, 2006, 1996, 3768, 1997, 1037, 2512, 25500, 16136, 3800, 2005, 1037, 5968, 1998, 1996, 2755, 2008, 1010, 2011, 2049, 2200, 3267, 1010, 2045, 2003, 1037, 6937, 3891, 2008, 1996, 5968, 2052, 2022, 2109, 2114, 1996, 2711, 2030, 3200, 1997, 2178, 1012, 2156, 2142, 2163, 1058, 1012, 10625, 1010, 8732, 1042, 1012, 7605, 6564, 2509, 1006, 6049, 25022, 2099, 1012, 2722, 1007, 1006, 19106, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 11898, 1010, 6391, 2575, 1042, 1012, 10514, 9397, 1012, 18596, 1010, 102, 3173, 2008, 2924, 13742, 2011, 2486, 1998, 4808, 2030, 28973, 2104, 2324, 15529, 19235, 2509, 2050, 2003, 1037, 4126, 1997, 4808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2852, 24065, 4887, 1521, 1055, 2522, 27794, 2015, 1010, 1996, 2522, 27794, 2052, 2022, 1037, 1523, 6778, 1524, 1997, 2437, 1996, 5968, 1012, 2582, 1010, 2543, 5092, 29232, 2024, 26096, 4795, 1012, 2045, 2003, 2053, 9379, 3800, 2005, 2437, 1037, 5968, 1012, 24648, 25173, 2008, 9125, 14792, 7515, 2004, 1523, 6355, 6997, 1524, 2005, 5682, 1997, 20226, 1996, 11746, 1997, 2476, 19591, 1012, 2156, 2324, 1057, 1012, 1055, 1012, 1039, 1012, 1073, 6227, 2549, 1006, 1041, 1007, 1006, 1016, 1007, 1006, 1038, 1007, 1006, 2462, 1007, 1006, 12854, 1037, 1523, 6355, 24648, 1524, 2004, 1024, 1523, 2151, 4126, 16385, 3085, 2011, 10219, 2005, 1037, 2744, 17003, 2028, 2095, 1012, 1012, 1012, 2008, 1012, 1012, 1012, 7336, 2224, 1997, 14792, 1524, 1007, 1012, 5434, 2031, 2179, 6664, 1997, 1037, 1005, 5968, 2000, 2022, 1037, 4126, 1997, 4808, 2241, 2006, 1996, 3768, 1997, 1037, 2512, 25500, 16136, 3800, 2005, 1037, 5968, 1998, 1996, 2755, 2008, 1010, 2011, 2049, 2200, 3267, 1010, 2045, 2003, 1037, 6937, 3891, 2008, 1996, 5968, 2052, 2022, 2109, 2114, 1996, 2711, 2030, 3200, 1997, 2178, 1012, 2156, 2142, 2163, 1058, 1012, 10625, 1010, 8732, 1042, 1012, 7605, 6564, 2509, 1006, 6049, 25022, 2099, 1012, 2722, 1007, 1006, 19106, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 11898, 1010, 6391, 2575, 1042, 1012, 10514, 9397, 1012, 18596, 1010, 102, 3173, 2008, 4424, 6101, 1997, 1037, 2775, 4591, 2004, 4126, 1997, 4808, 2104, 2324, 15529, 2385, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2852, 24065, 4887, 1521, 1055, 2522, 27794, 2015, 1010, 1996, 2522, 27794, 2052, 2022, 1037, 1523, 6778, 1524, 1997, 2437, 1996, 5968, 1012, 2582, 1010, 2543, 5092, 29232, 2024, 26096, 4795, 1012, 2045, 2003, 2053, 9379, 3800, 2005, 2437, 1037, 5968, 1012, 24648, 25173, 2008, 9125, 14792, 7515, 2004, 1523, 6355, 6997, 1524, 2005, 5682, 1997, 20226, 1996, 11746, 1997, 2476, 19591, 1012, 2156, 2324, 1057, 1012, 1055, 1012, 1039, 1012, 1073, 6227, 2549, 1006, 1041, 1007, 1006, 1016, 1007, 1006, 1038, 1007, 1006, 2462, 1007, 1006, 12854, 1037, 1523, 6355, 24648, 1524, 2004, 1024, 1523, 2151, 4126, 16385, 3085, 2011, 10219, 2005, 1037, 2744, 17003, 2028, 2095, 1012, 1012, 1012, 2008, 1012, 1012, 1012, 7336, 2224, 1997, 14792, 1524, 1007, 1012, 5434, 2031, 2179, 6664, 1997, 1037, 1005, 5968, 2000, 2022, 1037, 4126, 1997, 4808, 2241, 2006, 1996, 3768, 1997, 1037, 2512, 25500, 16136, 3800, 2005, 1037, 5968, 1998, 1996, 2755, 2008, 1010, 2011, 2049, 2200, 3267, 1010, 2045, 2003, 1037, 6937, 3891, 2008, 1996, 5968, 2052, 2022, 2109, 2114, 1996, 2711, 2030, 3200, 1997, 2178, 1012, 2156, 2142, 2163, 1058, 1012, 10625, 1010, 8732, 1042, 1012, 7605, 6564, 2509, 1006, 6049, 25022, 2099, 1012, 2722, 1007, 1006, 19106, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 11898, 1010, 6391, 2575, 1042, 1012, 10514, 102, 3173, 2005, 1996, 5682, 1997, 2324, 15529, 6227, 2549, 2063, 2008, 2108, 1037, 10768, 7811, 1999, 6664, 1997, 1037, 23646, 2003, 2025, 1037, 6355, 24648, 2004, 4225, 1999, 2324, 15529, 6227, 2549, 2063, 2475, 2497, 102], [101, 2852, 24065, 4887, 1521, 1055, 2522, 27794, 2015, 1010, 1996, 2522, 27794, 2052, 2022, 1037, 1523, 6778, 1524, 1997, 2437, 1996, 5968, 1012, 2582, 1010, 2543, 5092, 29232, 2024, 26096, 4795, 1012, 2045, 2003, 2053, 9379, 3800, 2005, 2437, 1037, 5968, 1012, 24648, 25173, 2008, 9125, 14792, 7515, 2004, 1523, 6355, 6997, 1524, 2005, 5682, 1997, 20226, 1996, 11746, 1997, 2476, 19591, 1012, 2156, 2324, 1057, 1012, 1055, 1012, 1039, 1012, 1073, 6227, 2549, 1006, 1041, 1007, 1006, 1016, 1007, 1006, 1038, 1007, 1006, 2462, 1007, 1006, 12854, 1037, 1523, 6355, 24648, 1524, 2004, 1024, 1523, 2151, 4126, 16385, 3085, 2011, 10219, 2005, 1037, 2744, 17003, 2028, 2095, 1012, 1012, 1012, 2008, 1012, 1012, 1012, 7336, 2224, 1997, 14792, 1524, 1007, 1012, 5434, 2031, 2179, 6664, 1997, 1037, 1005, 5968, 2000, 2022, 1037, 4126, 1997, 4808, 2241, 2006, 1996, 3768, 1997, 1037, 2512, 25500, 16136, 3800, 2005, 1037, 5968, 1998, 1996, 2755, 2008, 1010, 2011, 2049, 2200, 3267, 1010, 2045, 2003, 1037, 6937, 3891, 2008, 1996, 5968, 2052, 2022, 2109, 2114, 1996, 2711, 2030, 3200, 1997, 2178, 1012, 2156, 2142, 2163, 1058, 1012, 10625, 1010, 8732, 1042, 1012, 7605, 6564, 2509, 1006, 6049, 25022, 2099, 1012, 2722, 1007, 1006, 19106, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 11898, 1010, 6391, 2575, 102, 3173, 2008, 1037, 2457, 2442, 2069, 2298, 2000, 1996, 15201, 6210, 2025, 1996, 10318, 6214, 1997, 1996, 4126, 2000, 5646, 3251, 1037, 2445, 10048, 2003, 2011, 2049, 3267, 1037, 4126, 1997, 4808, 2005, 5682, 1997, 2324, 15529, 2385, 102]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], label=0)\n","06/08/2023 23:41:08 - INFO - casehold_helpers -   *** Example ***\n","06/08/2023 23:41:08 - INFO - casehold_helpers -   feature: InputFeatures(input_ids=[[101, 15270, 11368, 2050, 2109, 8013, 2592, 2008, 2002, 2165, 2013, 28567, 1012, 5678, 1010, 15270, 11368, 2050, 14456, 2000, 2383, 2579, 2012, 2560, 2048, 28567, 10340, 2007, 2032, 2000, 6104, 1012, 2023, 2828, 1997, 2592, 2089, 12346, 3119, 7800, 1012, 2156, 1043, 1012, 1048, 1012, 1039, 1012, 25162, 1010, 1073, 2382, 1006, 12854, 1523, 3119, 3595, 1524, 2004, 2109, 1999, 1043, 1012, 1048, 1012, 1039, 1012, 6109, 1010, 1073, 4413, 1010, 2004, 2164, 1523, 2505, 24600, 2030, 20014, 5654, 7028, 2030, 28926, 2921, 2030, 8250, 1010, 2029, 17367, 1010, 5836, 1010, 3350, 2015, 2030, 2636, 1037, 3595, 4045, 1010, 4087, 1010, 21442, 14856, 10521, 2075, 1010, 2537, 2030, 2968, 2592, 1010, 2640, 1010, 2832, 1010, 7709, 1010, 5675, 1010, 11028, 2030, 7620, 1524, 1007, 1025, 6654, 1011, 12838, 2522, 1012, 1010, 4413, 2581, 3742, 1012, 2012, 4749, 1006, 1523, 1031, 18777, 1998, 16350, 2449, 2592, 2089, 2022, 4709, 2000, 3860, 1010, 2130, 2065, 2107, 2592, 3685, 4366, 3119, 3595, 3860, 1524, 1007, 1025, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 15476, 4017, 1010, 4297, 1012, 1010, 2871, 2683, 3742, 1012, 2012, 19410, 1006, 1026, 3173, 1028, 1007, 1012, 1523, 5609, 1997, 2270, 3716, 2030, 1997, 2236, 102, 14622, 2008, 2130, 2065, 1037, 20579, 4447, 3056, 2592, 17367, 3119, 7800, 2049, 4366, 2089, 2025, 12530, 2006, 2008, 9128, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 15270, 11368, 2050, 2109, 8013, 2592, 2008, 2002, 2165, 2013, 28567, 1012, 5678, 1010, 15270, 11368, 2050, 14456, 2000, 2383, 2579, 2012, 2560, 2048, 28567, 10340, 2007, 2032, 2000, 6104, 1012, 2023, 2828, 1997, 2592, 2089, 12346, 3119, 7800, 1012, 2156, 1043, 1012, 1048, 1012, 1039, 1012, 25162, 1010, 1073, 2382, 1006, 12854, 1523, 3119, 3595, 1524, 2004, 2109, 1999, 1043, 1012, 1048, 1012, 1039, 1012, 6109, 1010, 1073, 4413, 1010, 2004, 2164, 1523, 2505, 24600, 2030, 20014, 5654, 7028, 2030, 28926, 2921, 2030, 8250, 1010, 2029, 17367, 1010, 5836, 1010, 3350, 2015, 2030, 2636, 1037, 3595, 4045, 1010, 4087, 1010, 21442, 14856, 10521, 2075, 1010, 2537, 2030, 2968, 2592, 1010, 2640, 1010, 2832, 1010, 7709, 1010, 5675, 1010, 11028, 2030, 7620, 1524, 1007, 1025, 6654, 1011, 12838, 2522, 1012, 1010, 4413, 2581, 3742, 1012, 2012, 4749, 1006, 1523, 1031, 18777, 1998, 16350, 2449, 2592, 2089, 2022, 4709, 2000, 3860, 1010, 2130, 2065, 2107, 2592, 3685, 4366, 3119, 3595, 3860, 1524, 1007, 1025, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 15476, 4017, 1010, 4297, 1012, 1010, 2871, 2683, 3742, 1012, 2012, 19410, 1006, 1026, 3173, 1028, 1007, 1012, 1523, 5609, 1997, 2270, 3716, 2030, 1997, 2236, 102, 3173, 2008, 2443, 2426, 3119, 7800, 7904, 2089, 2025, 6413, 2013, 11194, 2003, 3056, 2592, 2107, 2004, 7201, 1997, 6304, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 15270, 11368, 2050, 2109, 8013, 2592, 2008, 2002, 2165, 2013, 28567, 1012, 5678, 1010, 15270, 11368, 2050, 14456, 2000, 2383, 2579, 2012, 2560, 2048, 28567, 10340, 2007, 2032, 2000, 6104, 1012, 2023, 2828, 1997, 2592, 2089, 12346, 3119, 7800, 1012, 2156, 1043, 1012, 1048, 1012, 1039, 1012, 25162, 1010, 1073, 2382, 1006, 12854, 1523, 3119, 3595, 1524, 2004, 2109, 1999, 1043, 1012, 1048, 1012, 1039, 1012, 6109, 1010, 1073, 4413, 1010, 2004, 2164, 1523, 2505, 24600, 2030, 20014, 5654, 7028, 2030, 28926, 2921, 2030, 8250, 1010, 2029, 17367, 1010, 5836, 1010, 3350, 2015, 2030, 2636, 1037, 3595, 4045, 1010, 4087, 1010, 21442, 14856, 10521, 2075, 1010, 2537, 2030, 2968, 2592, 1010, 2640, 1010, 2832, 1010, 7709, 1010, 5675, 1010, 11028, 2030, 7620, 1524, 1007, 1025, 6654, 1011, 12838, 2522, 1012, 1010, 4413, 2581, 3742, 1012, 2012, 4749, 1006, 1523, 1031, 18777, 1998, 16350, 2449, 2592, 2089, 2022, 4709, 2000, 3860, 1010, 2130, 2065, 2107, 2592, 3685, 4366, 3119, 3595, 3860, 1524, 1007, 1025, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 15476, 4017, 1010, 4297, 1012, 1010, 2871, 2683, 3742, 1012, 2012, 19410, 1006, 1026, 3173, 1028, 1007, 1012, 1523, 5609, 1997, 2270, 3716, 2030, 1997, 2236, 102, 3173, 2008, 17024, 7201, 2064, 2022, 3119, 7800, 2104, 5242, 2015, 6375, 3119, 7800, 2552, 2029, 3594, 1996, 2168, 6210, 1997, 1037, 3119, 3595, 2004, 8124, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 15270, 11368, 2050, 2109, 8013, 2592, 2008, 2002, 2165, 2013, 28567, 1012, 5678, 1010, 15270, 11368, 2050, 14456, 2000, 2383, 2579, 2012, 2560, 2048, 28567, 10340, 2007, 2032, 2000, 6104, 1012, 2023, 2828, 1997, 2592, 2089, 12346, 3119, 7800, 1012, 2156, 1043, 1012, 1048, 1012, 1039, 1012, 25162, 1010, 1073, 2382, 1006, 12854, 1523, 3119, 3595, 1524, 2004, 2109, 1999, 1043, 1012, 1048, 1012, 1039, 1012, 6109, 1010, 1073, 4413, 1010, 2004, 2164, 1523, 2505, 24600, 2030, 20014, 5654, 7028, 2030, 28926, 2921, 2030, 8250, 1010, 2029, 17367, 1010, 5836, 1010, 3350, 2015, 2030, 2636, 1037, 3595, 4045, 1010, 4087, 1010, 21442, 14856, 10521, 2075, 1010, 2537, 2030, 2968, 2592, 1010, 2640, 1010, 2832, 1010, 7709, 1010, 5675, 1010, 11028, 2030, 7620, 1524, 1007, 1025, 6654, 1011, 12838, 2522, 1012, 1010, 4413, 2581, 3742, 1012, 2012, 4749, 1006, 1523, 1031, 18777, 1998, 16350, 2449, 2592, 2089, 2022, 4709, 2000, 3860, 1010, 2130, 2065, 2107, 2592, 3685, 4366, 3119, 3595, 3860, 1524, 1007, 1025, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 15476, 4017, 1010, 4297, 1012, 1010, 2871, 2683, 3742, 1012, 2012, 19410, 1006, 1026, 3173, 1028, 1007, 1012, 1523, 5609, 1997, 2270, 3716, 2030, 1997, 2236, 102, 14622, 2008, 8013, 7201, 2089, 2022, 4047, 3085, 3119, 7800, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 15270, 11368, 2050, 2109, 8013, 2592, 2008, 2002, 2165, 2013, 28567, 1012, 5678, 1010, 15270, 11368, 2050, 14456, 2000, 2383, 2579, 2012, 2560, 2048, 28567, 10340, 2007, 2032, 2000, 6104, 1012, 2023, 2828, 1997, 2592, 2089, 12346, 3119, 7800, 1012, 2156, 1043, 1012, 1048, 1012, 1039, 1012, 25162, 1010, 1073, 2382, 1006, 12854, 1523, 3119, 3595, 1524, 2004, 2109, 1999, 1043, 1012, 1048, 1012, 1039, 1012, 6109, 1010, 1073, 4413, 1010, 2004, 2164, 1523, 2505, 24600, 2030, 20014, 5654, 7028, 2030, 28926, 2921, 2030, 8250, 1010, 2029, 17367, 1010, 5836, 1010, 3350, 2015, 2030, 2636, 1037, 3595, 4045, 1010, 4087, 1010, 21442, 14856, 10521, 2075, 1010, 2537, 2030, 2968, 2592, 1010, 2640, 1010, 2832, 1010, 7709, 1010, 5675, 1010, 11028, 2030, 7620, 1524, 1007, 1025, 6654, 1011, 12838, 2522, 1012, 1010, 4413, 2581, 3742, 1012, 2012, 4749, 1006, 1523, 1031, 18777, 1998, 16350, 2449, 2592, 2089, 2022, 4709, 2000, 3860, 1010, 2130, 2065, 2107, 2592, 3685, 4366, 3119, 3595, 3860, 1524, 1007, 1025, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 15476, 4017, 1010, 4297, 1012, 1010, 2871, 2683, 3742, 1012, 2012, 19410, 1006, 1026, 3173, 1028, 1007, 1012, 1523, 5609, 1997, 2270, 3716, 2030, 1997, 2236, 102, 14622, 1037, 11476, 2342, 2000, 4047, 2019, 7904, 2013, 5860, 10483, 2075, 2019, 12433, 3119, 7800, 2030, 2060, 18777, 2592, 2000, 1037, 12692, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], label=1)\n","06/08/2023 23:41:08 - INFO - casehold_helpers -   Saving features into cached file .cache/case_hold/cached_train_BertBaseUncased_256_case_hold\n","06/08/2023 23:42:02 - WARNING - datasets.builder -   Found cached dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n","100% 3/3 [00:00<00:00, 637.27it/s]\n","06/08/2023 23:42:02 - INFO - casehold_helpers -   Creating features from dataset file at case_hold\n","06/08/2023 23:42:02 - INFO - casehold_helpers -   Training examples: 3900\n","convert examples to features: 0it [00:00, ?it/s]06/08/2023 23:42:02 - INFO - casehold_helpers -   Writing example 0 of 3900\n","convert examples to features: 3900it [00:20, 186.52it/s]\n","06/08/2023 23:42:23 - INFO - casehold_helpers -   *** Example ***\n","06/08/2023 23:42:23 - INFO - casehold_helpers -   feature: InputFeatures(input_ids=[[101, 4425, 1010, 20579, 1999, 2049, 12087, 1998, 1999, 2049, 4367, 2005, 12654, 8689, 2077, 1996, 10528, 2457, 22400, 27481, 2098, 2008, 1996, 12135, 8635, 4012, 6442, 2098, 2007, 1996, 15201, 5918, 1998, 11846, 1996, 1523, 2034, 1998, 2190, 4682, 2078, 1524, 2006, 19413, 2072, 1997, 13316, 1521, 1055, 7045, 1012, 20579, 13647, 2008, 1996, 15201, 1998, 2553, 2375, 2511, 2008, 1024, 1063, 1086, 2321, 1065, 1523, 1031, 1045, 1033, 23233, 16874, 4765, 18168, 14643, 3258, 1031, 1999, 23953, 12135, 8635, 1033, 2003, 2025, 5667, 22369, 1998, 2515, 2025, 17552, 1996, 12135, 8635, 19528, 2138, 1024, 1037, 1007, 1996, 8210, 18168, 14643, 3258, 1997, 1996, 2616, 1520, 1997, 2167, 3792, 1521, 2043, 1996, 11596, 1997, 19413, 2072, 1998, 19413, 2072, 1997, 13316, 2024, 1996, 2168, 1998, 1996, 2048, 2024, 3141, 3316, 2515, 2025, 2765, 1999, 1037, 22369, 7561, 1025, 1998, 1038, 1007, 1996, 4472, 21030, 3215, 4919, 6523, 1996, 8068, 2000, 1996, 4987, 3036, 3820, 2005, 1996, 8085, 1017, 15384, 2278, 16360, 1012, 14262, 2615, 1012, 6390, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1063, 1086, 2324, 1065, 20579, 19914, 1998, 1996, 10528, 102, 3173, 1037, 16158, 8085, 17367, 1996, 2224, 1997, 2008, 5381, 2171, 1998, 2947, 24209, 11475, 14213, 2004, 1037, 2965, 1997, 8720, 2104, 1996, 11671, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4425, 1010, 20579, 1999, 2049, 12087, 1998, 1999, 2049, 4367, 2005, 12654, 8689, 2077, 1996, 10528, 2457, 22400, 27481, 2098, 2008, 1996, 12135, 8635, 4012, 6442, 2098, 2007, 1996, 15201, 5918, 1998, 11846, 1996, 1523, 2034, 1998, 2190, 4682, 2078, 1524, 2006, 19413, 2072, 1997, 13316, 1521, 1055, 7045, 1012, 20579, 13647, 2008, 1996, 15201, 1998, 2553, 2375, 2511, 2008, 1024, 1063, 1086, 2321, 1065, 1523, 1031, 1045, 1033, 23233, 16874, 4765, 18168, 14643, 3258, 1031, 1999, 23953, 12135, 8635, 1033, 2003, 2025, 5667, 22369, 1998, 2515, 2025, 17552, 1996, 12135, 8635, 19528, 2138, 1024, 1037, 1007, 1996, 8210, 18168, 14643, 3258, 1997, 1996, 2616, 1520, 1997, 2167, 3792, 1521, 2043, 1996, 11596, 1997, 19413, 2072, 1998, 19413, 2072, 1997, 13316, 2024, 1996, 2168, 1998, 1996, 2048, 2024, 3141, 3316, 2515, 2025, 2765, 1999, 1037, 22369, 7561, 1025, 1998, 1038, 1007, 1996, 4472, 21030, 3215, 4919, 6523, 1996, 8068, 2000, 1996, 4987, 3036, 3820, 2005, 1996, 8085, 1017, 15384, 2278, 16360, 1012, 14262, 2615, 1012, 6390, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1063, 1086, 2324, 1065, 20579, 19914, 1998, 1996, 10528, 102, 14622, 2004, 1037, 3043, 1997, 2976, 2375, 2008, 2019, 2895, 2000, 2417, 8303, 6441, 2000, 1037, 3840, 3685, 2022, 5224, 2011, 1037, 18668, 1999, 2010, 2219, 2171, 2021, 2442, 2022, 2716, 1999, 1996, 2171, 1997, 1996, 3840, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4425, 1010, 20579, 1999, 2049, 12087, 1998, 1999, 2049, 4367, 2005, 12654, 8689, 2077, 1996, 10528, 2457, 22400, 27481, 2098, 2008, 1996, 12135, 8635, 4012, 6442, 2098, 2007, 1996, 15201, 5918, 1998, 11846, 1996, 1523, 2034, 1998, 2190, 4682, 2078, 1524, 2006, 19413, 2072, 1997, 13316, 1521, 1055, 7045, 1012, 20579, 13647, 2008, 1996, 15201, 1998, 2553, 2375, 2511, 2008, 1024, 1063, 1086, 2321, 1065, 1523, 1031, 1045, 1033, 23233, 16874, 4765, 18168, 14643, 3258, 1031, 1999, 23953, 12135, 8635, 1033, 2003, 2025, 5667, 22369, 1998, 2515, 2025, 17552, 1996, 12135, 8635, 19528, 2138, 1024, 1037, 1007, 1996, 8210, 18168, 14643, 3258, 1997, 1996, 2616, 1520, 1997, 2167, 3792, 1521, 2043, 1996, 11596, 1997, 19413, 2072, 1998, 19413, 2072, 1997, 13316, 2024, 1996, 2168, 1998, 1996, 2048, 2024, 3141, 3316, 2515, 2025, 2765, 1999, 1037, 22369, 7561, 1025, 1998, 1038, 1007, 1996, 4472, 21030, 3215, 4919, 6523, 1996, 8068, 2000, 1996, 4987, 3036, 3820, 2005, 1996, 8085, 1017, 15384, 2278, 16360, 1012, 14262, 2615, 1012, 6390, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1063, 1086, 2324, 1065, 20579, 19914, 1998, 1996, 10528, 102, 3173, 28966, 8720, 1997, 2711, 2011, 2171, 2001, 12949, 3563, 2005, 5682, 1997, 2353, 19362, 3723, 3841, 12879, 24108, 2854, 3570, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4425, 1010, 20579, 1999, 2049, 12087, 1998, 1999, 2049, 4367, 2005, 12654, 8689, 2077, 1996, 10528, 2457, 22400, 27481, 2098, 2008, 1996, 12135, 8635, 4012, 6442, 2098, 2007, 1996, 15201, 5918, 1998, 11846, 1996, 1523, 2034, 1998, 2190, 4682, 2078, 1524, 2006, 19413, 2072, 1997, 13316, 1521, 1055, 7045, 1012, 20579, 13647, 2008, 1996, 15201, 1998, 2553, 2375, 2511, 2008, 1024, 1063, 1086, 2321, 1065, 1523, 1031, 1045, 1033, 23233, 16874, 4765, 18168, 14643, 3258, 1031, 1999, 23953, 12135, 8635, 1033, 2003, 2025, 5667, 22369, 1998, 2515, 2025, 17552, 1996, 12135, 8635, 19528, 2138, 1024, 1037, 1007, 1996, 8210, 18168, 14643, 3258, 1997, 1996, 2616, 1520, 1997, 2167, 3792, 1521, 2043, 1996, 11596, 1997, 19413, 2072, 1998, 19413, 2072, 1997, 13316, 2024, 1996, 2168, 1998, 1996, 2048, 2024, 3141, 3316, 2515, 2025, 2765, 1999, 1037, 22369, 7561, 1025, 1998, 1038, 1007, 1996, 4472, 21030, 3215, 4919, 6523, 1996, 8068, 2000, 1996, 4987, 3036, 3820, 2005, 1996, 8085, 1017, 15384, 2278, 16360, 1012, 14262, 2615, 1012, 6390, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1063, 1086, 2324, 1065, 20579, 19914, 1998, 1996, 10528, 102, 3173, 2000, 2022, 7182, 1996, 16542, 8720, 1997, 7016, 2953, 2004, 28005, 6519, 2522, 2612, 1997, 1996, 6149, 2171, 1997, 2888, 28005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4425, 1010, 20579, 1999, 2049, 12087, 1998, 1999, 2049, 4367, 2005, 12654, 8689, 2077, 1996, 10528, 2457, 22400, 27481, 2098, 2008, 1996, 12135, 8635, 4012, 6442, 2098, 2007, 1996, 15201, 5918, 1998, 11846, 1996, 1523, 2034, 1998, 2190, 4682, 2078, 1524, 2006, 19413, 2072, 1997, 13316, 1521, 1055, 7045, 1012, 20579, 13647, 2008, 1996, 15201, 1998, 2553, 2375, 2511, 2008, 1024, 1063, 1086, 2321, 1065, 1523, 1031, 1045, 1033, 23233, 16874, 4765, 18168, 14643, 3258, 1031, 1999, 23953, 12135, 8635, 1033, 2003, 2025, 5667, 22369, 1998, 2515, 2025, 17552, 1996, 12135, 8635, 19528, 2138, 1024, 1037, 1007, 1996, 8210, 18168, 14643, 3258, 1997, 1996, 2616, 1520, 1997, 2167, 3792, 1521, 2043, 1996, 11596, 1997, 19413, 2072, 1998, 19413, 2072, 1997, 13316, 2024, 1996, 2168, 1998, 1996, 2048, 2024, 3141, 3316, 2515, 2025, 2765, 1999, 1037, 22369, 7561, 1025, 1998, 1038, 1007, 1996, 4472, 21030, 3215, 4919, 6523, 1996, 8068, 2000, 1996, 4987, 3036, 3820, 2005, 1996, 8085, 1017, 15384, 2278, 16360, 1012, 14262, 2615, 1012, 6390, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1063, 1086, 2324, 1065, 20579, 19914, 1998, 1996, 10528, 102, 3173, 2008, 2019, 2895, 2716, 1999, 1996, 2171, 1997, 1037, 2711, 4748, 9103, 26022, 2004, 27523, 19498, 15198, 2612, 1997, 1999, 1996, 2171, 1997, 2014, 6697, 3685, 2022, 13371, 2004, 1037, 28616, 3630, 5017, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], label=3)\n","06/08/2023 23:42:23 - INFO - casehold_helpers -   *** Example ***\n","06/08/2023 23:42:23 - INFO - casehold_helpers -   feature: InputFeatures(input_ids=[[101, 2031, 2979, 2046, 1996, 2398, 1997, 14753, 2050, 26000, 23112, 2030, 5309, 2869, 2005, 3643, 1010, 2004, 2146, 2004, 2151, 13930, 1997, 1996, 3840, 2024, 23850, 1010, 1996, 13304, 1997, 1996, 7045, 2202, 2068, 5338, 2007, 1037, 3404, 1999, 5684, 1997, 1996, 23112, 1524, 1007, 1025, 5785, 1058, 1012, 2103, 1997, 3996, 1010, 16065, 1055, 1012, 1039, 1012, 4868, 2575, 1010, 15471, 1055, 1012, 1041, 1012, 3963, 2629, 1010, 6390, 2475, 1006, 4662, 1007, 1006, 1523, 1031, 1056, 1033, 2002, 2200, 2617, 1037, 3840, 1010, 8169, 2030, 2060, 1010, 6561, 1996, 2391, 1997, 16021, 4747, 8159, 5666, 1006, 3653, 17421, 24971, 2135, 1010, 2004, 1037, 3043, 1997, 2607, 1010, 2000, 1996, 3716, 1997, 1996, 6605, 6074, 1997, 1996, 3840, 1007, 1010, 5121, 1999, 1996, 6234, 9824, 1997, 12275, 1998, 10528, 1010, 2049, 7045, 2468, 7622, 2007, 1037, 19487, 3404, 2000, 2022, 5500, 9350, 8231, 2426, 2049, 23112, 1010, 3395, 1010, 1997, 2607, 1010, 2000, 4682, 3619, 1025, 1998, 1996, 6605, 6074, 2468, 1996, 15631, 1997, 2008, 3404, 1012, 1524, 1007, 1025, 2156, 2036, 5954, 1058, 1012, 10882, 19766, 1010, 16528, 1055, 1012, 1039, 1012, 4413, 2549, 1010, 17332, 1055, 1012, 1041, 1012, 17943, 1010, 13913, 1006, 4612, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 12310, 3536, 4013, 5104, 1012, 1010, 4297, 1012, 1058, 1012, 8076, 1010, 24232, 102, 14622, 1996, 2157, 1997, 1037, 4923, 2953, 2000, 9790, 1037, 5971, 2472, 2005, 12510, 1997, 10882, 8566, 7405, 2854, 4611, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2031, 2979, 2046, 1996, 2398, 1997, 14753, 2050, 26000, 23112, 2030, 5309, 2869, 2005, 3643, 1010, 2004, 2146, 2004, 2151, 13930, 1997, 1996, 3840, 2024, 23850, 1010, 1996, 13304, 1997, 1996, 7045, 2202, 2068, 5338, 2007, 1037, 3404, 1999, 5684, 1997, 1996, 23112, 1524, 1007, 1025, 5785, 1058, 1012, 2103, 1997, 3996, 1010, 16065, 1055, 1012, 1039, 1012, 4868, 2575, 1010, 15471, 1055, 1012, 1041, 1012, 3963, 2629, 1010, 6390, 2475, 1006, 4662, 1007, 1006, 1523, 1031, 1056, 1033, 2002, 2200, 2617, 1037, 3840, 1010, 8169, 2030, 2060, 1010, 6561, 1996, 2391, 1997, 16021, 4747, 8159, 5666, 1006, 3653, 17421, 24971, 2135, 1010, 2004, 1037, 3043, 1997, 2607, 1010, 2000, 1996, 3716, 1997, 1996, 6605, 6074, 1997, 1996, 3840, 1007, 1010, 5121, 1999, 1996, 6234, 9824, 1997, 12275, 1998, 10528, 1010, 2049, 7045, 2468, 7622, 2007, 1037, 19487, 3404, 2000, 2022, 5500, 9350, 8231, 2426, 2049, 23112, 1010, 3395, 1010, 1997, 2607, 1010, 2000, 4682, 3619, 1025, 1998, 1996, 6605, 6074, 2468, 1996, 15631, 1997, 2008, 3404, 1012, 1524, 1007, 1025, 2156, 2036, 5954, 1058, 1012, 10882, 19766, 1010, 16528, 1055, 1012, 1039, 1012, 4413, 2549, 1010, 17332, 1055, 1012, 1041, 1012, 17943, 1010, 13913, 1006, 4612, 1007, 1006, 1026, 3173, 102, 3173, 2008, 1037, 20579, 6224, 3265, 4335, 2104, 9413, 14268, 2753, 2475, 2050, 2509, 2104, 1037, 12510, 1997, 10882, 8566, 7405, 2854, 4611, 3399, 2106, 2025, 2031, 1037, 3426, 1997, 2895, 2043, 1996, 6884, 12510, 1997, 10882, 8566, 7405, 2854, 4611, 2001, 1037, 4945, 2000, 16062, 6666, 1999, 10388, 2007, 1996, 2933, 102], [101, 2031, 2979, 2046, 1996, 2398, 1997, 14753, 2050, 26000, 23112, 2030, 5309, 2869, 2005, 3643, 1010, 2004, 2146, 2004, 2151, 13930, 1997, 1996, 3840, 2024, 23850, 1010, 1996, 13304, 1997, 1996, 7045, 2202, 2068, 5338, 2007, 1037, 3404, 1999, 5684, 1997, 1996, 23112, 1524, 1007, 1025, 5785, 1058, 1012, 2103, 1997, 3996, 1010, 16065, 1055, 1012, 1039, 1012, 4868, 2575, 1010, 15471, 1055, 1012, 1041, 1012, 3963, 2629, 1010, 6390, 2475, 1006, 4662, 1007, 1006, 1523, 1031, 1056, 1033, 2002, 2200, 2617, 1037, 3840, 1010, 8169, 2030, 2060, 1010, 6561, 1996, 2391, 1997, 16021, 4747, 8159, 5666, 1006, 3653, 17421, 24971, 2135, 1010, 2004, 1037, 3043, 1997, 2607, 1010, 2000, 1996, 3716, 1997, 1996, 6605, 6074, 1997, 1996, 3840, 1007, 1010, 5121, 1999, 1996, 6234, 9824, 1997, 12275, 1998, 10528, 1010, 2049, 7045, 2468, 7622, 2007, 1037, 19487, 3404, 2000, 2022, 5500, 9350, 8231, 2426, 2049, 23112, 1010, 3395, 1010, 1997, 2607, 1010, 2000, 4682, 3619, 1025, 1998, 1996, 6605, 6074, 2468, 1996, 15631, 1997, 2008, 3404, 1012, 1524, 1007, 1025, 2156, 2036, 5954, 1058, 1012, 10882, 19766, 1010, 16528, 1055, 1012, 1039, 1012, 4413, 2549, 1010, 17332, 1055, 1012, 1041, 1012, 17943, 1010, 13913, 1006, 4612, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 12310, 3536, 4013, 5104, 1012, 1010, 4297, 1012, 1058, 1012, 8076, 1010, 24232, 102, 3173, 2008, 12510, 1997, 10882, 8566, 7405, 2854, 4611, 4366, 2001, 3653, 6633, 13876, 2098, 2011, 10768, 2232, 3676, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2031, 2979, 2046, 1996, 2398, 1997, 14753, 2050, 26000, 23112, 2030, 5309, 2869, 2005, 3643, 1010, 2004, 2146, 2004, 2151, 13930, 1997, 1996, 3840, 2024, 23850, 1010, 1996, 13304, 1997, 1996, 7045, 2202, 2068, 5338, 2007, 1037, 3404, 1999, 5684, 1997, 1996, 23112, 1524, 1007, 1025, 5785, 1058, 1012, 2103, 1997, 3996, 1010, 16065, 1055, 1012, 1039, 1012, 4868, 2575, 1010, 15471, 1055, 1012, 1041, 1012, 3963, 2629, 1010, 6390, 2475, 1006, 4662, 1007, 1006, 1523, 1031, 1056, 1033, 2002, 2200, 2617, 1037, 3840, 1010, 8169, 2030, 2060, 1010, 6561, 1996, 2391, 1997, 16021, 4747, 8159, 5666, 1006, 3653, 17421, 24971, 2135, 1010, 2004, 1037, 3043, 1997, 2607, 1010, 2000, 1996, 3716, 1997, 1996, 6605, 6074, 1997, 1996, 3840, 1007, 1010, 5121, 1999, 1996, 6234, 9824, 1997, 12275, 1998, 10528, 1010, 2049, 7045, 2468, 7622, 2007, 1037, 19487, 3404, 2000, 2022, 5500, 9350, 8231, 2426, 2049, 23112, 1010, 3395, 1010, 1997, 2607, 1010, 2000, 4682, 3619, 1025, 1998, 1996, 6605, 6074, 2468, 1996, 15631, 1997, 2008, 3404, 1012, 1524, 1007, 1025, 2156, 2036, 5954, 1058, 1012, 10882, 19766, 1010, 16528, 1055, 1012, 1039, 1012, 4413, 2549, 1010, 17332, 1055, 1012, 1041, 1012, 17943, 1010, 13913, 1006, 4612, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 12310, 3536, 4013, 5104, 1012, 1010, 4297, 1012, 1058, 1012, 8076, 1010, 24232, 102, 3173, 1037, 3426, 1997, 2895, 2005, 12510, 1997, 10882, 8566, 7405, 2854, 4611, 2097, 2025, 4682, 2073, 1996, 4366, 1997, 12510, 2003, 7790, 2588, 1996, 4598, 1997, 1037, 27948, 3276, 2090, 1996, 4243, 102, 0, 0, 0], [101, 2031, 2979, 2046, 1996, 2398, 1997, 14753, 2050, 26000, 23112, 2030, 5309, 2869, 2005, 3643, 1010, 2004, 2146, 2004, 2151, 13930, 1997, 1996, 3840, 2024, 23850, 1010, 1996, 13304, 1997, 1996, 7045, 2202, 2068, 5338, 2007, 1037, 3404, 1999, 5684, 1997, 1996, 23112, 1524, 1007, 1025, 5785, 1058, 1012, 2103, 1997, 3996, 1010, 16065, 1055, 1012, 1039, 1012, 4868, 2575, 1010, 15471, 1055, 1012, 1041, 1012, 3963, 2629, 1010, 6390, 2475, 1006, 4662, 1007, 1006, 1523, 1031, 1056, 1033, 2002, 2200, 2617, 1037, 3840, 1010, 8169, 2030, 2060, 1010, 6561, 1996, 2391, 1997, 16021, 4747, 8159, 5666, 1006, 3653, 17421, 24971, 2135, 1010, 2004, 1037, 3043, 1997, 2607, 1010, 2000, 1996, 3716, 1997, 1996, 6605, 6074, 1997, 1996, 3840, 1007, 1010, 5121, 1999, 1996, 6234, 9824, 1997, 12275, 1998, 10528, 1010, 2049, 7045, 2468, 7622, 2007, 1037, 19487, 3404, 2000, 2022, 5500, 9350, 8231, 2426, 2049, 23112, 1010, 3395, 1010, 1997, 2607, 1010, 2000, 4682, 3619, 1025, 1998, 1996, 6605, 6074, 2468, 1996, 15631, 1997, 2008, 3404, 1012, 1524, 1007, 1025, 2156, 2036, 5954, 1058, 1012, 10882, 19766, 1010, 16528, 1055, 1012, 1039, 1012, 4413, 2549, 1010, 17332, 1055, 1012, 1041, 1012, 17943, 1010, 13913, 1006, 4612, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 12310, 3536, 4013, 5104, 1012, 1010, 4297, 1012, 1058, 1012, 8076, 1010, 24232, 102, 3173, 2008, 5284, 2375, 4162, 2000, 1996, 23953, 12510, 1997, 10882, 8566, 7405, 2854, 4611, 4366, 2138, 2009, 2003, 5971, 2375, 2008, 11859, 1996, 9530, 21163, 2015, 1997, 2008, 4611, 102, 0, 0, 0, 0, 0, 0]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], label=0)\n","06/08/2023 23:42:23 - INFO - casehold_helpers -   Saving features into cached file .cache/case_hold/cached_dev_BertBaseUncased_256_case_hold\n","06/08/2023 23:42:30 - WARNING - datasets.builder -   Found cached dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n","100% 3/3 [00:00<00:00, 652.71it/s]\n","06/08/2023 23:42:30 - INFO - casehold_helpers -   Creating features from dataset file at case_hold\n","06/08/2023 23:42:30 - INFO - casehold_helpers -   Training examples: 3600\n","convert examples to features: 0it [00:00, ?it/s]06/08/2023 23:42:30 - INFO - casehold_helpers -   Writing example 0 of 3600\n","convert examples to features: 3600it [00:17, 203.02it/s]\n","06/08/2023 23:42:47 - INFO - casehold_helpers -   *** Example ***\n","06/08/2023 23:42:47 - INFO - casehold_helpers -   feature: InputFeatures(input_ids=[[101, 1058, 1012, 2373, 8740, 2705, 1012, 1997, 1996, 2110, 1997, 2047, 2259, 1010, 6282, 1050, 1012, 1061, 1012, 14134, 4185, 2683, 1010, 3515, 2475, 1010, 3438, 2475, 1050, 1012, 1061, 1012, 1055, 1012, 14134, 5388, 2620, 1010, 5786, 2487, 1050, 1012, 1041, 1012, 14134, 13285, 2629, 1010, 13285, 2575, 1006, 1050, 1012, 1061, 1012, 2857, 1007, 1006, 2168, 1007, 1012, 2122, 3572, 6570, 2008, 4189, 3006, 3643, 2003, 2025, 2000, 2022, 4340, 1999, 1037, 4678, 10451, 8391, 1997, 10061, 17208, 1010, 2021, 2013, 1996, 7339, 1997, 1037, 25613, 17634, 1999, 1996, 2613, 2088, 1517, 2130, 2065, 2014, 10617, 1997, 3643, 2003, 2241, 6822, 2006, 28616, 2378, 14192, 3370, 2030, 1006, 15835, 1007, 16903, 10069, 2055, 9662, 2925, 2824, 1012, 2060, 5434, 1010, 1999, 1996, 2168, 2030, 2714, 6214, 1010, 2031, 2218, 2008, 1996, 11162, 1997, 3445, 11099, 2030, 4022, 22010, 2006, 1996, 2455, 3588, 2044, 1037, 2635, 1010, 2029, 2028, 2457, 2038, 12599, 1523, 26453, 12394, 1010, 1524, 2064, 2022, 2641, 1999, 20077, 7367, 21998, 12394, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 9207, 13117, 1010, 1048, 1012, 1052, 1012, 1010, 1058, 1012, 6273, 1012, 4583, 4631, 1997, 2455, 1010, 17403, 1042, 1012, 10514, 9397, 1012, 14134, 6365, 2683, 1010, 5345, 2581, 1006, 1050, 1012, 1040, 1012, 5665, 1012, 2541, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 2403, 1012, 4229, 4631, 1997, 2455, 1010, 3770, 1042, 1012, 7605, 102, 3173, 2008, 5025, 12394, 2104, 24501, 4502, 2443, 6832, 12394, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1058, 1012, 2373, 8740, 2705, 1012, 1997, 1996, 2110, 1997, 2047, 2259, 1010, 6282, 1050, 1012, 1061, 1012, 14134, 4185, 2683, 1010, 3515, 2475, 1010, 3438, 2475, 1050, 1012, 1061, 1012, 1055, 1012, 14134, 5388, 2620, 1010, 5786, 2487, 1050, 1012, 1041, 1012, 14134, 13285, 2629, 1010, 13285, 2575, 1006, 1050, 1012, 1061, 1012, 2857, 1007, 1006, 2168, 1007, 1012, 2122, 3572, 6570, 2008, 4189, 3006, 3643, 2003, 2025, 2000, 2022, 4340, 1999, 1037, 4678, 10451, 8391, 1997, 10061, 17208, 1010, 2021, 2013, 1996, 7339, 1997, 1037, 25613, 17634, 1999, 1996, 2613, 2088, 1517, 2130, 2065, 2014, 10617, 1997, 3643, 2003, 2241, 6822, 2006, 28616, 2378, 14192, 3370, 2030, 1006, 15835, 1007, 16903, 10069, 2055, 9662, 2925, 2824, 1012, 2060, 5434, 1010, 1999, 1996, 2168, 2030, 2714, 6214, 1010, 2031, 2218, 2008, 1996, 11162, 1997, 3445, 11099, 2030, 4022, 22010, 2006, 1996, 2455, 3588, 2044, 1037, 2635, 1010, 2029, 2028, 2457, 2038, 12599, 1523, 26453, 12394, 1010, 1524, 2064, 2022, 2641, 1999, 20077, 7367, 21998, 12394, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 9207, 13117, 1010, 1048, 1012, 1052, 1012, 1010, 1058, 1012, 6273, 1012, 4583, 4631, 1997, 2455, 1010, 17403, 1042, 1012, 10514, 9397, 1012, 14134, 6365, 2683, 1010, 5345, 2581, 1006, 1050, 1012, 1040, 1012, 5665, 1012, 2541, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 2403, 1012, 4229, 4631, 1997, 2455, 1010, 3770, 1042, 1012, 7605, 102, 3173, 2008, 3445, 3571, 1997, 9451, 2001, 1037, 3043, 2008, 2755, 23695, 2071, 7919, 5136, 1999, 20077, 7367, 21998, 12394, 102], [101, 1058, 1012, 2373, 8740, 2705, 1012, 1997, 1996, 2110, 1997, 2047, 2259, 1010, 6282, 1050, 1012, 1061, 1012, 14134, 4185, 2683, 1010, 3515, 2475, 1010, 3438, 2475, 1050, 1012, 1061, 1012, 1055, 1012, 14134, 5388, 2620, 1010, 5786, 2487, 1050, 1012, 1041, 1012, 14134, 13285, 2629, 1010, 13285, 2575, 1006, 1050, 1012, 1061, 1012, 2857, 1007, 1006, 2168, 1007, 1012, 2122, 3572, 6570, 2008, 4189, 3006, 3643, 2003, 2025, 2000, 2022, 4340, 1999, 1037, 4678, 10451, 8391, 1997, 10061, 17208, 1010, 2021, 2013, 1996, 7339, 1997, 1037, 25613, 17634, 1999, 1996, 2613, 2088, 1517, 2130, 2065, 2014, 10617, 1997, 3643, 2003, 2241, 6822, 2006, 28616, 2378, 14192, 3370, 2030, 1006, 15835, 1007, 16903, 10069, 2055, 9662, 2925, 2824, 1012, 2060, 5434, 1010, 1999, 1996, 2168, 2030, 2714, 6214, 1010, 2031, 2218, 2008, 1996, 11162, 1997, 3445, 11099, 2030, 4022, 22010, 2006, 1996, 2455, 3588, 2044, 1037, 2635, 1010, 2029, 2028, 2457, 2038, 12599, 1523, 26453, 12394, 1010, 1524, 2064, 2022, 2641, 1999, 20077, 7367, 21998, 12394, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 9207, 13117, 1010, 1048, 1012, 1052, 1012, 1010, 1058, 1012, 6273, 1012, 4583, 4631, 1997, 2455, 1010, 17403, 1042, 1012, 10514, 9397, 1012, 14134, 6365, 2683, 1010, 5345, 2581, 1006, 1050, 1012, 1040, 1012, 5665, 1012, 2541, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 2403, 1012, 4229, 4631, 1997, 2455, 1010, 3770, 1042, 1012, 102, 3173, 2008, 2334, 17394, 3571, 1997, 2825, 14161, 9331, 18041, 1999, 14081, 3370, 11476, 2135, 5360, 17208, 1997, 7367, 21998, 12394, 102], [101, 1058, 1012, 2373, 8740, 2705, 1012, 1997, 1996, 2110, 1997, 2047, 2259, 1010, 6282, 1050, 1012, 1061, 1012, 14134, 4185, 2683, 1010, 3515, 2475, 1010, 3438, 2475, 1050, 1012, 1061, 1012, 1055, 1012, 14134, 5388, 2620, 1010, 5786, 2487, 1050, 1012, 1041, 1012, 14134, 13285, 2629, 1010, 13285, 2575, 1006, 1050, 1012, 1061, 1012, 2857, 1007, 1006, 2168, 1007, 1012, 2122, 3572, 6570, 2008, 4189, 3006, 3643, 2003, 2025, 2000, 2022, 4340, 1999, 1037, 4678, 10451, 8391, 1997, 10061, 17208, 1010, 2021, 2013, 1996, 7339, 1997, 1037, 25613, 17634, 1999, 1996, 2613, 2088, 1517, 2130, 2065, 2014, 10617, 1997, 3643, 2003, 2241, 6822, 2006, 28616, 2378, 14192, 3370, 2030, 1006, 15835, 1007, 16903, 10069, 2055, 9662, 2925, 2824, 1012, 2060, 5434, 1010, 1999, 1996, 2168, 2030, 2714, 6214, 1010, 2031, 2218, 2008, 1996, 11162, 1997, 3445, 11099, 2030, 4022, 22010, 2006, 1996, 2455, 3588, 2044, 1037, 2635, 1010, 2029, 2028, 2457, 2038, 12599, 1523, 26453, 12394, 1010, 1524, 2064, 2022, 2641, 1999, 20077, 7367, 21998, 12394, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 9207, 13117, 1010, 1048, 1012, 1052, 1012, 1010, 1058, 1012, 6273, 1012, 4583, 4631, 1997, 2455, 1010, 17403, 1042, 1012, 10514, 9397, 1012, 14134, 6365, 2683, 1010, 5345, 2581, 1006, 1050, 1012, 1040, 1012, 5665, 1012, 2541, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 2403, 1012, 4229, 4631, 1997, 2455, 1010, 3770, 1042, 1012, 7605, 102, 14622, 4145, 1997, 26453, 12394, 2004, 2027, 6611, 2000, 3200, 2029, 2003, 1999, 2755, 19450, 102, 0, 0, 0, 0, 0], [101, 1058, 1012, 2373, 8740, 2705, 1012, 1997, 1996, 2110, 1997, 2047, 2259, 1010, 6282, 1050, 1012, 1061, 1012, 14134, 4185, 2683, 1010, 3515, 2475, 1010, 3438, 2475, 1050, 1012, 1061, 1012, 1055, 1012, 14134, 5388, 2620, 1010, 5786, 2487, 1050, 1012, 1041, 1012, 14134, 13285, 2629, 1010, 13285, 2575, 1006, 1050, 1012, 1061, 1012, 2857, 1007, 1006, 2168, 1007, 1012, 2122, 3572, 6570, 2008, 4189, 3006, 3643, 2003, 2025, 2000, 2022, 4340, 1999, 1037, 4678, 10451, 8391, 1997, 10061, 17208, 1010, 2021, 2013, 1996, 7339, 1997, 1037, 25613, 17634, 1999, 1996, 2613, 2088, 1517, 2130, 2065, 2014, 10617, 1997, 3643, 2003, 2241, 6822, 2006, 28616, 2378, 14192, 3370, 2030, 1006, 15835, 1007, 16903, 10069, 2055, 9662, 2925, 2824, 1012, 2060, 5434, 1010, 1999, 1996, 2168, 2030, 2714, 6214, 1010, 2031, 2218, 2008, 1996, 11162, 1997, 3445, 11099, 2030, 4022, 22010, 2006, 1996, 2455, 3588, 2044, 1037, 2635, 1010, 2029, 2028, 2457, 2038, 12599, 1523, 26453, 12394, 1010, 1524, 2064, 2022, 2641, 1999, 20077, 7367, 21998, 12394, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 9207, 13117, 1010, 1048, 1012, 1052, 1012, 1010, 1058, 1012, 6273, 1012, 4583, 4631, 1997, 2455, 1010, 17403, 1042, 1012, 10514, 9397, 1012, 14134, 6365, 2683, 1010, 5345, 2581, 1006, 1050, 1012, 1040, 1012, 5665, 1012, 2541, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 2142, 2163, 1058, 1012, 2403, 1012, 4229, 4631, 1997, 102, 3173, 2008, 26453, 12394, 2020, 7919, 2443, 1999, 17208, 1997, 7367, 21998, 12394, 2073, 10039, 1997, 3806, 13117, 2071, 9495, 10069, 2055, 2825, 28616, 3270, 4523, 102]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], label=4)\n","06/08/2023 23:42:48 - INFO - casehold_helpers -   *** Example ***\n","06/08/2023 23:42:48 - INFO - casehold_helpers -   feature: InputFeatures(input_ids=[[101, 1997, 1073, 3486, 2620, 2509, 1006, 1041, 1007, 1006, 1017, 1007, 2001, 16286, 18921, 19763, 3085, 1998, 3024, 1996, 13474, 2007, 1037, 4189, 5432, 1012, 2947, 1010, 2009, 2001, 2025, 20454, 2000, 6611, 3779, 22307, 19620, 2135, 1012, 2348, 13945, 2003, 19106, 1010, 1998, 2947, 2025, 8031, 1010, 13945, 2003, 23949, 1998, 2566, 6342, 21369, 3726, 1012, 3568, 1010, 11243, 3779, 22307, 19620, 2135, 2000, 10337, 1521, 1055, 2857, 10652, 2515, 2025, 23640, 1996, 2349, 2832, 11075, 1010, 1998, 1996, 2212, 2457, 2106, 2025, 24250, 9413, 2099, 1999, 24964, 8737, 18606, 13588, 2713, 2044, 1996, 2034, 7065, 23909, 1012, 11914, 1010, 10337, 1521, 1055, 6251, 2003, 19768, 1012, 19768, 1025, 4367, 7219, 2004, 9587, 4140, 1012, 1015, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 2142, 2163, 1058, 1012, 2751, 2075, 1010, 6421, 2683, 1042, 1012, 14134, 18677, 1010, 19681, 1006, 4833, 25022, 2099, 1012, 3118, 1007, 1012, 1016, 1012, 17710, 10649, 2819, 1058, 1012, 6084, 3514, 13058, 1012, 1010, 6535, 2620, 1042, 1012, 14134, 18914, 1010, 17832, 1006, 4833, 25022, 2099, 1012, 3069, 1007, 1012, 1017, 1012, 2156, 1041, 5677, 10686, 1058, 1012, 2142, 2163, 1010, 5139, 2575, 1057, 1012, 1055, 1012, 2260, 1010, 14010, 1055, 1012, 14931, 1012, 28203, 1010, 27433, 1011, 5718, 1010, 17867, 1048, 1012, 3968, 1012, 14134, 2403, 1006, 2384, 1007, 1006, 2566, 12731, 25557, 1007, 1006, 1026, 3173, 1028, 1007, 1025, 12849, 3372, 11285, 1058, 1012, 4575, 1010, 20263, 1057, 1012, 1055, 1012, 4008, 2509, 102, 3173, 2008, 1996, 16362, 3350, 2106, 2025, 7515, 2004, 4397, 3603, 3350, 102], [101, 1997, 1073, 3486, 2620, 2509, 1006, 1041, 1007, 1006, 1017, 1007, 2001, 16286, 18921, 19763, 3085, 1998, 3024, 1996, 13474, 2007, 1037, 4189, 5432, 1012, 2947, 1010, 2009, 2001, 2025, 20454, 2000, 6611, 3779, 22307, 19620, 2135, 1012, 2348, 13945, 2003, 19106, 1010, 1998, 2947, 2025, 8031, 1010, 13945, 2003, 23949, 1998, 2566, 6342, 21369, 3726, 1012, 3568, 1010, 11243, 3779, 22307, 19620, 2135, 2000, 10337, 1521, 1055, 2857, 10652, 2515, 2025, 23640, 1996, 2349, 2832, 11075, 1010, 1998, 1996, 2212, 2457, 2106, 2025, 24250, 9413, 2099, 1999, 24964, 8737, 18606, 13588, 2713, 2044, 1996, 2034, 7065, 23909, 1012, 11914, 1010, 10337, 1521, 1055, 6251, 2003, 19768, 1012, 19768, 1025, 4367, 7219, 2004, 9587, 4140, 1012, 1015, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 2142, 2163, 1058, 1012, 2751, 2075, 1010, 6421, 2683, 1042, 1012, 14134, 18677, 1010, 19681, 1006, 4833, 25022, 2099, 1012, 3118, 1007, 1012, 1016, 1012, 17710, 10649, 2819, 1058, 1012, 6084, 3514, 13058, 1012, 1010, 6535, 2620, 1042, 1012, 14134, 18914, 1010, 17832, 1006, 4833, 25022, 2099, 1012, 3069, 1007, 1012, 1017, 1012, 2156, 1041, 5677, 10686, 1058, 1012, 2142, 2163, 1010, 5139, 2575, 1057, 1012, 1055, 1012, 2260, 1010, 14010, 1055, 1012, 14931, 1012, 28203, 1010, 27433, 1011, 5718, 1010, 17867, 1048, 1012, 3968, 1012, 14134, 2403, 1006, 2384, 1007, 1006, 2566, 12731, 25557, 102, 3173, 5066, 2110, 9964, 2005, 2695, 8663, 7903, 3508, 4335, 2029, 2001, 2241, 2006, 4397, 3603, 3350, 2021, 5837, 2011, 1996, 2110, 5434, 2138, 1996, 3350, 2001, 2025, 4397, 3603, 2001, 7919, 6406, 102], [101, 1997, 1073, 3486, 2620, 2509, 1006, 1041, 1007, 1006, 1017, 1007, 2001, 16286, 18921, 19763, 3085, 1998, 3024, 1996, 13474, 2007, 1037, 4189, 5432, 1012, 2947, 1010, 2009, 2001, 2025, 20454, 2000, 6611, 3779, 22307, 19620, 2135, 1012, 2348, 13945, 2003, 19106, 1010, 1998, 2947, 2025, 8031, 1010, 13945, 2003, 23949, 1998, 2566, 6342, 21369, 3726, 1012, 3568, 1010, 11243, 3779, 22307, 19620, 2135, 2000, 10337, 1521, 1055, 2857, 10652, 2515, 2025, 23640, 1996, 2349, 2832, 11075, 1010, 1998, 1996, 2212, 2457, 2106, 2025, 24250, 9413, 2099, 1999, 24964, 8737, 18606, 13588, 2713, 2044, 1996, 2034, 7065, 23909, 1012, 11914, 1010, 10337, 1521, 1055, 6251, 2003, 19768, 1012, 19768, 1025, 4367, 7219, 2004, 9587, 4140, 1012, 1015, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 2142, 2163, 1058, 1012, 2751, 2075, 1010, 6421, 2683, 1042, 1012, 14134, 18677, 1010, 19681, 1006, 4833, 25022, 2099, 1012, 3118, 1007, 1012, 1016, 1012, 17710, 10649, 2819, 1058, 1012, 6084, 3514, 13058, 1012, 1010, 6535, 2620, 1042, 1012, 14134, 18914, 1010, 17832, 1006, 4833, 25022, 2099, 1012, 3069, 1007, 1012, 1017, 1012, 2156, 1041, 5677, 10686, 1058, 1012, 2142, 2163, 1010, 5139, 2575, 1057, 1012, 1055, 1012, 2260, 1010, 14010, 1055, 1012, 14931, 1012, 28203, 1010, 27433, 1011, 5718, 1010, 17867, 1048, 1012, 3968, 1012, 14134, 102, 3173, 2008, 3513, 4292, 5743, 2051, 6537, 2005, 1037, 16362, 4367, 2005, 1037, 2047, 3979, 16764, 2006, 1037, 3114, 2060, 2084, 4397, 3603, 3350, 2024, 2025, 7360, 2389, 2021, 2612, 2024, 2512, 9103, 6935, 29201, 19301, 4366, 21572, 9623, 7741, 3513, 102], [101, 1997, 1073, 3486, 2620, 2509, 1006, 1041, 1007, 1006, 1017, 1007, 2001, 16286, 18921, 19763, 3085, 1998, 3024, 1996, 13474, 2007, 1037, 4189, 5432, 1012, 2947, 1010, 2009, 2001, 2025, 20454, 2000, 6611, 3779, 22307, 19620, 2135, 1012, 2348, 13945, 2003, 19106, 1010, 1998, 2947, 2025, 8031, 1010, 13945, 2003, 23949, 1998, 2566, 6342, 21369, 3726, 1012, 3568, 1010, 11243, 3779, 22307, 19620, 2135, 2000, 10337, 1521, 1055, 2857, 10652, 2515, 2025, 23640, 1996, 2349, 2832, 11075, 1010, 1998, 1996, 2212, 2457, 2106, 2025, 24250, 9413, 2099, 1999, 24964, 8737, 18606, 13588, 2713, 2044, 1996, 2034, 7065, 23909, 1012, 11914, 1010, 10337, 1521, 1055, 6251, 2003, 19768, 1012, 19768, 1025, 4367, 7219, 2004, 9587, 4140, 1012, 1015, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 2142, 2163, 1058, 1012, 2751, 2075, 1010, 6421, 2683, 1042, 1012, 14134, 18677, 1010, 19681, 1006, 4833, 25022, 2099, 1012, 3118, 1007, 1012, 1016, 1012, 17710, 10649, 2819, 1058, 1012, 6084, 3514, 13058, 1012, 1010, 6535, 2620, 1042, 1012, 14134, 18914, 1010, 17832, 1006, 4833, 25022, 2099, 1012, 3069, 1007, 1012, 1017, 1012, 2156, 1041, 5677, 10686, 1058, 1012, 2142, 2163, 1010, 5139, 2575, 1057, 1012, 1055, 1012, 2260, 1010, 14010, 1055, 1012, 14931, 1012, 28203, 1010, 27433, 1011, 5718, 1010, 17867, 1048, 1012, 3968, 1012, 14134, 2403, 1006, 2384, 1007, 1006, 2566, 12731, 25557, 1007, 1006, 1026, 3173, 1028, 102, 3173, 2008, 4397, 3603, 3350, 2442, 2022, 2008, 2029, 5839, 2012, 1996, 2051, 1997, 3979, 2021, 2005, 2019, 4654, 7874, 3085, 3114, 2001, 2025, 7523, 3085, 2127, 2101, 102], [101, 1997, 1073, 3486, 2620, 2509, 1006, 1041, 1007, 1006, 1017, 1007, 2001, 16286, 18921, 19763, 3085, 1998, 3024, 1996, 13474, 2007, 1037, 4189, 5432, 1012, 2947, 1010, 2009, 2001, 2025, 20454, 2000, 6611, 3779, 22307, 19620, 2135, 1012, 2348, 13945, 2003, 19106, 1010, 1998, 2947, 2025, 8031, 1010, 13945, 2003, 23949, 1998, 2566, 6342, 21369, 3726, 1012, 3568, 1010, 11243, 3779, 22307, 19620, 2135, 2000, 10337, 1521, 1055, 2857, 10652, 2515, 2025, 23640, 1996, 2349, 2832, 11075, 1010, 1998, 1996, 2212, 2457, 2106, 2025, 24250, 9413, 2099, 1999, 24964, 8737, 18606, 13588, 2713, 2044, 1996, 2034, 7065, 23909, 1012, 11914, 1010, 10337, 1521, 1055, 6251, 2003, 19768, 1012, 19768, 1025, 4367, 7219, 2004, 9587, 4140, 1012, 1015, 1012, 2156, 1010, 1041, 1012, 1043, 1012, 1010, 2142, 2163, 1058, 1012, 2751, 2075, 1010, 6421, 2683, 1042, 1012, 14134, 18677, 1010, 19681, 1006, 4833, 25022, 2099, 1012, 3118, 1007, 1012, 1016, 1012, 17710, 10649, 2819, 1058, 1012, 6084, 3514, 13058, 1012, 1010, 6535, 2620, 1042, 1012, 14134, 18914, 1010, 17832, 1006, 4833, 25022, 2099, 1012, 3069, 1007, 1012, 1017, 1012, 2156, 1041, 5677, 10686, 1058, 1012, 2142, 2163, 1010, 5139, 2575, 1057, 1012, 1055, 1012, 2260, 1010, 14010, 1055, 1012, 14931, 1012, 28203, 1010, 27433, 1011, 5718, 1010, 17867, 1048, 1012, 3968, 1012, 14134, 2403, 1006, 2384, 1007, 1006, 2566, 12731, 25557, 1007, 102, 3173, 2008, 1996, 2212, 2457, 2106, 2025, 6905, 2049, 19258, 1999, 16039, 4367, 2005, 2047, 3979, 2241, 2006, 4397, 3603, 3350, 2073, 1996, 3350, 2052, 3710, 2069, 2000, 17727, 5243, 2818, 10896, 102]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], label=2)\n","06/08/2023 23:42:48 - INFO - casehold_helpers -   Saving features into cached file .cache/case_hold/cached_test_BertBaseUncased_256_case_hold\n","06/08/2023 23:42:52 - INFO - __main__ -   Sample 8805 of the training set: InputFeatures(input_ids=[[101, 1996, 3043, 1997, 13003, 1037, 6467, 2003, 15142, 2000, 1996, 3277, 1997, 3251, 2019, 2592, 3640, 11706, 5060, 2000, 1037, 4735, 13474, 1012, 1999, 2010, 2117, 6685, 1010, 21964, 27481, 2098, 2008, 2010, 4367, 2000, 24209, 11823, 1996, 2592, 2323, 2031, 2042, 4379, 2349, 2000, 1996, 13727, 2791, 1997, 1996, 7655, 1523, 10439, 15204, 3401, 1012, 1524, 2057, 21090, 1012, 1996, 2592, 1999, 2023, 2553, 2163, 2008, 21964, 1523, 5296, 4968, 1998, 2512, 1011, 4968, 5949, 2164, 2019, 10439, 15204, 3401, 1998, 5992, 7318, 1012, 1524, 1996, 3146, 3831, 3642, 11859, 1996, 2744, 1523, 4968, 5949, 1010, 1524, 1998, 2009, 2582, 27171, 2008, 1523, 5992, 7318, 1524, 1998, 1523, 22449, 1524, 2024, 2512, 1011, 4968, 5949, 2015, 2008, 3685, 2022, 5296, 1012, 2156, 2382, 16060, 1012, 4748, 10020, 1012, 3642, 1073, 1073, 7886, 1012, 1015, 1006, 2656, 1007, 1010, 11118, 1012, 19348, 1006, 1015, 1007, 1012, 2004, 2107, 1010, 2122, 3408, 2106, 2025, 5478, 2582, 16418, 1999, 1996, 2592, 1012, 2156, 2726, 1058, 1012, 2110, 1010, 5786, 2487, 1055, 1012, 1059, 1012, 14134, 17696, 1010, 17365, 1006, 16060, 1012, 13675, 5714, 1012, 10439, 1012, 3261, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 3568, 1010, 2138, 1996, 3408, 1523, 10439, 15204, 3401, 1524, 1998, 102, 3173, 2008, 2043, 1037, 2744, 2003, 4225, 1999, 1996, 11671, 2009, 2342, 2025, 2022, 2582, 6884, 1999, 1996, 24265, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 3043, 1997, 13003, 1037, 6467, 2003, 15142, 2000, 1996, 3277, 1997, 3251, 2019, 2592, 3640, 11706, 5060, 2000, 1037, 4735, 13474, 1012, 1999, 2010, 2117, 6685, 1010, 21964, 27481, 2098, 2008, 2010, 4367, 2000, 24209, 11823, 1996, 2592, 2323, 2031, 2042, 4379, 2349, 2000, 1996, 13727, 2791, 1997, 1996, 7655, 1523, 10439, 15204, 3401, 1012, 1524, 2057, 21090, 1012, 1996, 2592, 1999, 2023, 2553, 2163, 2008, 21964, 1523, 5296, 4968, 1998, 2512, 1011, 4968, 5949, 2164, 2019, 10439, 15204, 3401, 1998, 5992, 7318, 1012, 1524, 1996, 3146, 3831, 3642, 11859, 1996, 2744, 1523, 4968, 5949, 1010, 1524, 1998, 2009, 2582, 27171, 2008, 1523, 5992, 7318, 1524, 1998, 1523, 22449, 1524, 2024, 2512, 1011, 4968, 5949, 2015, 2008, 3685, 2022, 5296, 1012, 2156, 2382, 16060, 1012, 4748, 10020, 1012, 3642, 1073, 1073, 7886, 1012, 1015, 1006, 2656, 1007, 1010, 11118, 1012, 19348, 1006, 1015, 1007, 1012, 2004, 2107, 1010, 2122, 3408, 2106, 2025, 5478, 2582, 16418, 1999, 1996, 2592, 1012, 2156, 2726, 1058, 1012, 2110, 1010, 5786, 2487, 1055, 1012, 1059, 1012, 14134, 17696, 1010, 17365, 1006, 16060, 1012, 13675, 5714, 1012, 10439, 1012, 3261, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 3568, 1010, 2138, 1996, 3408, 1523, 10439, 15204, 3401, 1524, 1998, 102, 3173, 1996, 3853, 1997, 1037, 2270, 9710, 2003, 9756, 2025, 2129, 1996, 2744, 2003, 4225, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 3043, 1997, 13003, 1037, 6467, 2003, 15142, 2000, 1996, 3277, 1997, 3251, 2019, 2592, 3640, 11706, 5060, 2000, 1037, 4735, 13474, 1012, 1999, 2010, 2117, 6685, 1010, 21964, 27481, 2098, 2008, 2010, 4367, 2000, 24209, 11823, 1996, 2592, 2323, 2031, 2042, 4379, 2349, 2000, 1996, 13727, 2791, 1997, 1996, 7655, 1523, 10439, 15204, 3401, 1012, 1524, 2057, 21090, 1012, 1996, 2592, 1999, 2023, 2553, 2163, 2008, 21964, 1523, 5296, 4968, 1998, 2512, 1011, 4968, 5949, 2164, 2019, 10439, 15204, 3401, 1998, 5992, 7318, 1012, 1524, 1996, 3146, 3831, 3642, 11859, 1996, 2744, 1523, 4968, 5949, 1010, 1524, 1998, 2009, 2582, 27171, 2008, 1523, 5992, 7318, 1524, 1998, 1523, 22449, 1524, 2024, 2512, 1011, 4968, 5949, 2015, 2008, 3685, 2022, 5296, 1012, 2156, 2382, 16060, 1012, 4748, 10020, 1012, 3642, 1073, 1073, 7886, 1012, 1015, 1006, 2656, 1007, 1010, 11118, 1012, 19348, 1006, 1015, 1007, 1012, 2004, 2107, 1010, 2122, 3408, 2106, 2025, 5478, 2582, 16418, 1999, 1996, 2592, 1012, 2156, 2726, 1058, 1012, 2110, 1010, 5786, 2487, 1055, 1012, 1059, 1012, 14134, 17696, 1010, 17365, 1006, 16060, 1012, 13675, 5714, 1012, 10439, 1012, 3261, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 3568, 1010, 2138, 1996, 3408, 1523, 10439, 15204, 3401, 1524, 1998, 102, 3173, 2008, 2348, 2744, 2001, 2025, 3132, 2011, 1996, 12827, 2009, 2001, 4671, 2135, 4225, 1999, 1037, 4867, 5450, 1999, 1996, 11537, 2381, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 3043, 1997, 13003, 1037, 6467, 2003, 15142, 2000, 1996, 3277, 1997, 3251, 2019, 2592, 3640, 11706, 5060, 2000, 1037, 4735, 13474, 1012, 1999, 2010, 2117, 6685, 1010, 21964, 27481, 2098, 2008, 2010, 4367, 2000, 24209, 11823, 1996, 2592, 2323, 2031, 2042, 4379, 2349, 2000, 1996, 13727, 2791, 1997, 1996, 7655, 1523, 10439, 15204, 3401, 1012, 1524, 2057, 21090, 1012, 1996, 2592, 1999, 2023, 2553, 2163, 2008, 21964, 1523, 5296, 4968, 1998, 2512, 1011, 4968, 5949, 2164, 2019, 10439, 15204, 3401, 1998, 5992, 7318, 1012, 1524, 1996, 3146, 3831, 3642, 11859, 1996, 2744, 1523, 4968, 5949, 1010, 1524, 1998, 2009, 2582, 27171, 2008, 1523, 5992, 7318, 1524, 1998, 1523, 22449, 1524, 2024, 2512, 1011, 4968, 5949, 2015, 2008, 3685, 2022, 5296, 1012, 2156, 2382, 16060, 1012, 4748, 10020, 1012, 3642, 1073, 1073, 7886, 1012, 1015, 1006, 2656, 1007, 1010, 11118, 1012, 19348, 1006, 1015, 1007, 1012, 2004, 2107, 1010, 2122, 3408, 2106, 2025, 5478, 2582, 16418, 1999, 1996, 2592, 1012, 2156, 2726, 1058, 1012, 2110, 1010, 5786, 2487, 1055, 1012, 1059, 1012, 14134, 17696, 1010, 17365, 1006, 16060, 1012, 13675, 5714, 1012, 10439, 1012, 3261, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 3568, 1010, 2138, 1996, 3408, 1523, 10439, 15204, 3401, 1524, 1998, 102, 3173, 2008, 12340, 6313, 25033, 3619, 8197, 16259, 14000, 2342, 2025, 2022, 5338, 1999, 1996, 24265, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 3043, 1997, 13003, 1037, 6467, 2003, 15142, 2000, 1996, 3277, 1997, 3251, 2019, 2592, 3640, 11706, 5060, 2000, 1037, 4735, 13474, 1012, 1999, 2010, 2117, 6685, 1010, 21964, 27481, 2098, 2008, 2010, 4367, 2000, 24209, 11823, 1996, 2592, 2323, 2031, 2042, 4379, 2349, 2000, 1996, 13727, 2791, 1997, 1996, 7655, 1523, 10439, 15204, 3401, 1012, 1524, 2057, 21090, 1012, 1996, 2592, 1999, 2023, 2553, 2163, 2008, 21964, 1523, 5296, 4968, 1998, 2512, 1011, 4968, 5949, 2164, 2019, 10439, 15204, 3401, 1998, 5992, 7318, 1012, 1524, 1996, 3146, 3831, 3642, 11859, 1996, 2744, 1523, 4968, 5949, 1010, 1524, 1998, 2009, 2582, 27171, 2008, 1523, 5992, 7318, 1524, 1998, 1523, 22449, 1524, 2024, 2512, 1011, 4968, 5949, 2015, 2008, 3685, 2022, 5296, 1012, 2156, 2382, 16060, 1012, 4748, 10020, 1012, 3642, 1073, 1073, 7886, 1012, 1015, 1006, 2656, 1007, 1010, 11118, 1012, 19348, 1006, 1015, 1007, 1012, 2004, 2107, 1010, 2122, 3408, 2106, 2025, 5478, 2582, 16418, 1999, 1996, 2592, 1012, 2156, 2726, 1058, 1012, 2110, 1010, 5786, 2487, 1055, 1012, 1059, 1012, 14134, 17696, 1010, 17365, 1006, 16060, 1012, 13675, 5714, 1012, 10439, 1012, 3261, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 3568, 1010, 2138, 1996, 3408, 1523, 10439, 15204, 3401, 1524, 1998, 102, 3173, 2008, 2043, 1037, 2744, 2003, 2025, 4225, 1999, 1037, 3206, 1996, 3653, 17421, 16790, 2003, 2008, 1996, 2744, 2003, 2000, 2022, 2445, 2049, 6623, 3574, 1998, 7784, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], label=0).\n","06/08/2023 23:42:52 - INFO - __main__ -   Sample 37303 of the training set: InputFeatures(input_ids=[[101, 1017, 1012, 7297, 1010, 2016, 27481, 2015, 2008, 2014, 4905, 1521, 1055, 10741, 8170, 3397, 7182, 6947, 1997, 2014, 9517, 1521, 1055, 15644, 1010, 1998, 2008, 2016, 2038, 8510, 2014, 10859, 1997, 7411, 1996, 9608, 2791, 1997, 2014, 7303, 6165, 1012, 8909, 1012, 2012, 1017, 1011, 1018, 1012, 1996, 2283, 17942, 4905, 1521, 1055, 9883, 2442, 12040, 3350, 4760, 1523, 1996, 16214, 1521, 25640, 6078, 1025, 1996, 16214, 1521, 8066, 1010, 3325, 1010, 1998, 5891, 1025, 1998, 1996, 19283, 3006, 6165, 1999, 1996, 7882, 2451, 1012, 1524, 2156, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2581, 1006, 8951, 14154, 2213, 1058, 1012, 26261, 15551, 1010, 4805, 2629, 1057, 1012, 1055, 1012, 6070, 2575, 1010, 6486, 2575, 1050, 1012, 2340, 1010, 9645, 1055, 1012, 14931, 1012, 16666, 2487, 1010, 6535, 1048, 1012, 3968, 1012, 14134, 6486, 2487, 1006, 3118, 1007, 1007, 1012, 1996, 19283, 3006, 3446, 1999, 1996, 2474, 24513, 8185, 2003, 1523, 2021, 2028, 1997, 1996, 3787, 2734, 2000, 5323, 1996, 9608, 2791, 1997, 1037, 25640, 3446, 4912, 1999, 1037, 7408, 4646, 1012, 1524, 4027, 1010, 6353, 2575, 1042, 1012, 10514, 9397, 1012, 14134, 2012, 9645, 1025, 2156, 2036, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1996, 19283, 3006, 3446, 1523, 3073, 1031, 1055, 1033, 6414, 102, 3173, 3979, 2457, 9413, 5596, 1999, 21467, 16214, 9883, 2000, 11572, 1999, 6438, 1997, 2151, 3350, 1997, 16214, 9883, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1017, 1012, 7297, 1010, 2016, 27481, 2015, 2008, 2014, 4905, 1521, 1055, 10741, 8170, 3397, 7182, 6947, 1997, 2014, 9517, 1521, 1055, 15644, 1010, 1998, 2008, 2016, 2038, 8510, 2014, 10859, 1997, 7411, 1996, 9608, 2791, 1997, 2014, 7303, 6165, 1012, 8909, 1012, 2012, 1017, 1011, 1018, 1012, 1996, 2283, 17942, 4905, 1521, 1055, 9883, 2442, 12040, 3350, 4760, 1523, 1996, 16214, 1521, 25640, 6078, 1025, 1996, 16214, 1521, 8066, 1010, 3325, 1010, 1998, 5891, 1025, 1998, 1996, 19283, 3006, 6165, 1999, 1996, 7882, 2451, 1012, 1524, 2156, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2581, 1006, 8951, 14154, 2213, 1058, 1012, 26261, 15551, 1010, 4805, 2629, 1057, 1012, 1055, 1012, 6070, 2575, 1010, 6486, 2575, 1050, 1012, 2340, 1010, 9645, 1055, 1012, 14931, 1012, 16666, 2487, 1010, 6535, 1048, 1012, 3968, 1012, 14134, 6486, 2487, 1006, 3118, 1007, 1007, 1012, 1996, 19283, 3006, 3446, 1999, 1996, 2474, 24513, 8185, 2003, 1523, 2021, 2028, 1997, 1996, 3787, 2734, 2000, 5323, 1996, 9608, 2791, 1997, 1037, 25640, 3446, 4912, 1999, 1037, 7408, 4646, 1012, 1524, 4027, 1010, 6353, 2575, 1042, 1012, 10514, 9397, 1012, 14134, 2012, 9645, 1025, 2156, 2036, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1996, 19283, 3006, 3446, 1523, 3073, 1031, 1055, 1033, 6414, 102, 3173, 2008, 1037, 25640, 3446, 1999, 10388, 2007, 1996, 2474, 24513, 8185, 2001, 9608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1017, 1012, 7297, 1010, 2016, 27481, 2015, 2008, 2014, 4905, 1521, 1055, 10741, 8170, 3397, 7182, 6947, 1997, 2014, 9517, 1521, 1055, 15644, 1010, 1998, 2008, 2016, 2038, 8510, 2014, 10859, 1997, 7411, 1996, 9608, 2791, 1997, 2014, 7303, 6165, 1012, 8909, 1012, 2012, 1017, 1011, 1018, 1012, 1996, 2283, 17942, 4905, 1521, 1055, 9883, 2442, 12040, 3350, 4760, 1523, 1996, 16214, 1521, 25640, 6078, 1025, 1996, 16214, 1521, 8066, 1010, 3325, 1010, 1998, 5891, 1025, 1998, 1996, 19283, 3006, 6165, 1999, 1996, 7882, 2451, 1012, 1524, 2156, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2581, 1006, 8951, 14154, 2213, 1058, 1012, 26261, 15551, 1010, 4805, 2629, 1057, 1012, 1055, 1012, 6070, 2575, 1010, 6486, 2575, 1050, 1012, 2340, 1010, 9645, 1055, 1012, 14931, 1012, 16666, 2487, 1010, 6535, 1048, 1012, 3968, 1012, 14134, 6486, 2487, 1006, 3118, 1007, 1007, 1012, 1996, 19283, 3006, 3446, 1999, 1996, 2474, 24513, 8185, 2003, 1523, 2021, 2028, 1997, 1996, 3787, 2734, 2000, 5323, 1996, 9608, 2791, 1997, 1037, 25640, 3446, 4912, 1999, 1037, 7408, 4646, 1012, 1524, 4027, 1010, 6353, 2575, 1042, 1012, 10514, 9397, 1012, 14134, 2012, 9645, 1025, 2156, 2036, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1996, 19283, 3006, 3446, 1523, 3073, 1031, 1055, 1033, 6414, 102, 3173, 2008, 1996, 2474, 24513, 8185, 2003, 1996, 5372, 5675, 2000, 5646, 1996, 19283, 3006, 3446, 2005, 3423, 2578, 10155, 1999, 4434, 2007, 2801, 3831, 8931, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1017, 1012, 7297, 1010, 2016, 27481, 2015, 2008, 2014, 4905, 1521, 1055, 10741, 8170, 3397, 7182, 6947, 1997, 2014, 9517, 1521, 1055, 15644, 1010, 1998, 2008, 2016, 2038, 8510, 2014, 10859, 1997, 7411, 1996, 9608, 2791, 1997, 2014, 7303, 6165, 1012, 8909, 1012, 2012, 1017, 1011, 1018, 1012, 1996, 2283, 17942, 4905, 1521, 1055, 9883, 2442, 12040, 3350, 4760, 1523, 1996, 16214, 1521, 25640, 6078, 1025, 1996, 16214, 1521, 8066, 1010, 3325, 1010, 1998, 5891, 1025, 1998, 1996, 19283, 3006, 6165, 1999, 1996, 7882, 2451, 1012, 1524, 2156, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2581, 1006, 8951, 14154, 2213, 1058, 1012, 26261, 15551, 1010, 4805, 2629, 1057, 1012, 1055, 1012, 6070, 2575, 1010, 6486, 2575, 1050, 1012, 2340, 1010, 9645, 1055, 1012, 14931, 1012, 16666, 2487, 1010, 6535, 1048, 1012, 3968, 1012, 14134, 6486, 2487, 1006, 3118, 1007, 1007, 1012, 1996, 19283, 3006, 3446, 1999, 1996, 2474, 24513, 8185, 2003, 1523, 2021, 2028, 1997, 1996, 3787, 2734, 2000, 5323, 1996, 9608, 2791, 1997, 1037, 25640, 3446, 4912, 1999, 1037, 7408, 4646, 1012, 1524, 4027, 1010, 6353, 2575, 1042, 1012, 10514, 9397, 1012, 14134, 2012, 9645, 1025, 2156, 2036, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1996, 19283, 3006, 3446, 1523, 3073, 1031, 1055, 1033, 6414, 102, 3173, 2008, 2801, 3831, 8931, 2029, 5478, 6739, 10896, 4953, 3251, 1037, 3076, 2038, 2042, 6380, 1037, 6904, 5051, 2024, 12949, 3375, 2000, 10943, 4646, 1997, 1996, 2474, 24513, 8185, 102, 0, 0, 0, 0, 0, 0], [101, 1017, 1012, 7297, 1010, 2016, 27481, 2015, 2008, 2014, 4905, 1521, 1055, 10741, 8170, 3397, 7182, 6947, 1997, 2014, 9517, 1521, 1055, 15644, 1010, 1998, 2008, 2016, 2038, 8510, 2014, 10859, 1997, 7411, 1996, 9608, 2791, 1997, 2014, 7303, 6165, 1012, 8909, 1012, 2012, 1017, 1011, 1018, 1012, 1996, 2283, 17942, 4905, 1521, 1055, 9883, 2442, 12040, 3350, 4760, 1523, 1996, 16214, 1521, 25640, 6078, 1025, 1996, 16214, 1521, 8066, 1010, 3325, 1010, 1998, 5891, 1025, 1998, 1996, 19283, 3006, 6165, 1999, 1996, 7882, 2451, 1012, 1524, 2156, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2581, 1006, 8951, 14154, 2213, 1058, 1012, 26261, 15551, 1010, 4805, 2629, 1057, 1012, 1055, 1012, 6070, 2575, 1010, 6486, 2575, 1050, 1012, 2340, 1010, 9645, 1055, 1012, 14931, 1012, 16666, 2487, 1010, 6535, 1048, 1012, 3968, 1012, 14134, 6486, 2487, 1006, 3118, 1007, 1007, 1012, 1996, 19283, 3006, 3446, 1999, 1996, 2474, 24513, 8185, 2003, 1523, 2021, 2028, 1997, 1996, 3787, 2734, 2000, 5323, 1996, 9608, 2791, 1997, 1037, 25640, 3446, 4912, 1999, 1037, 7408, 4646, 1012, 1524, 4027, 1010, 6353, 2575, 1042, 1012, 10514, 9397, 1012, 14134, 2012, 9645, 1025, 2156, 2036, 2522, 21827, 1010, 5401, 1042, 1012, 7605, 2012, 7287, 2683, 1006, 1026, 3173, 1028, 1007, 1012, 1996, 19283, 3006, 3446, 1523, 3073, 1031, 1055, 1033, 6414, 102, 3173, 2008, 23953, 2089, 3073, 3350, 2000, 12448, 1996, 2474, 24513, 8185, 2164, 9883, 3018, 2000, 16214, 2007, 2714, 15644, 1999, 12435, 3572, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], label=4).\n","06/08/2023 23:42:52 - INFO - __main__ -   Sample 4135 of the training set: InputFeatures(input_ids=[[101, 1997, 5427, 1012, 2008, 11075, 4919, 2163, 2008, 1996, 13474, 2003, 27885, 14715, 3064, 2000, 2025, 8757, 20579, 1997, 1037, 1523, 16990, 1524, 1997, 1996, 3343, 1012, 16990, 2003, 2200, 2367, 2084, 4654, 16781, 1997, 1996, 3343, 1012, 2104, 1996, 3448, 18574, 1010, 13474, 2003, 2069, 27885, 14715, 3064, 2000, 5653, 2019, 16021, 12165, 2019, 3749, 2000, 20687, 4983, 13474, 2038, 2787, 2025, 2000, 20687, 1996, 3343, 1012, 13474, 3668, 2000, 16021, 12165, 2019, 3749, 2000, 20687, 1012, 16021, 12165, 2106, 2025, 5138, 1998, 1010, 3568, 1010, 1037, 2047, 3206, 1997, 5427, 2001, 2025, 2719, 1012, 2045, 2003, 2053, 2653, 1999, 1996, 3279, 3477, 3085, 11075, 2008, 5942, 13474, 2000, 2025, 8757, 20579, 1997, 16021, 12165, 1521, 1055, 4945, 2000, 20687, 1996, 3343, 1012, 1996, 2702, 1011, 2154, 5060, 9347, 21208, 2015, 2069, 16990, 1010, 2029, 2003, 1996, 18287, 1997, 6325, 2076, 1996, 3343, 2558, 1012, 2156, 13619, 25810, 1058, 1012, 5336, 16021, 1012, 2522, 1012, 1997, 2662, 1010, 4868, 12436, 1012, 25022, 2099, 1012, 2861, 1006, 2639, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 2008, 9347, 2515, 2025, 6611, 1999, 2023, 2553, 102, 3173, 2008, 1996, 23953, 13647, 3423, 3978, 2005, 6325, 2003, 22537, 2000, 1996, 9128, 1997, 3251, 1996, 5427, 3343, 3640, 6325, 1998, 2612, 2559, 2000, 1996, 8866, 10318, 1996, 4366, 2005, 6325, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1997, 5427, 1012, 2008, 11075, 4919, 2163, 2008, 1996, 13474, 2003, 27885, 14715, 3064, 2000, 2025, 8757, 20579, 1997, 1037, 1523, 16990, 1524, 1997, 1996, 3343, 1012, 16990, 2003, 2200, 2367, 2084, 4654, 16781, 1997, 1996, 3343, 1012, 2104, 1996, 3448, 18574, 1010, 13474, 2003, 2069, 27885, 14715, 3064, 2000, 5653, 2019, 16021, 12165, 2019, 3749, 2000, 20687, 4983, 13474, 2038, 2787, 2025, 2000, 20687, 1996, 3343, 1012, 13474, 3668, 2000, 16021, 12165, 2019, 3749, 2000, 20687, 1012, 16021, 12165, 2106, 2025, 5138, 1998, 1010, 3568, 1010, 1037, 2047, 3206, 1997, 5427, 2001, 2025, 2719, 1012, 2045, 2003, 2053, 2653, 1999, 1996, 3279, 3477, 3085, 11075, 2008, 5942, 13474, 2000, 2025, 8757, 20579, 1997, 16021, 12165, 1521, 1055, 4945, 2000, 20687, 1996, 3343, 1012, 1996, 2702, 1011, 2154, 5060, 9347, 21208, 2015, 2069, 16990, 1010, 2029, 2003, 1996, 18287, 1997, 6325, 2076, 1996, 3343, 2558, 1012, 2156, 13619, 25810, 1058, 1012, 5336, 16021, 1012, 2522, 1012, 1997, 2662, 1010, 4868, 12436, 1012, 25022, 2099, 1012, 2861, 1006, 2639, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 2008, 9347, 2515, 2025, 6611, 1999, 2023, 2553, 102, 3173, 2008, 2065, 2019, 16021, 27595, 23439, 6325, 2241, 2006, 2019, 23617, 2008, 1996, 10318, 4366, 2003, 12421, 2013, 6325, 2045, 2003, 1037, 3653, 17421, 16790, 2008, 1996, 16021, 27595, 2106, 2025, 9015, 18024, 2138, 25732, 5060, 2052, 2031, 6414, 4504, 1999, 2019, 3041, 14920, 1997, 6325, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1997, 5427, 1012, 2008, 11075, 4919, 2163, 2008, 1996, 13474, 2003, 27885, 14715, 3064, 2000, 2025, 8757, 20579, 1997, 1037, 1523, 16990, 1524, 1997, 1996, 3343, 1012, 16990, 2003, 2200, 2367, 2084, 4654, 16781, 1997, 1996, 3343, 1012, 2104, 1996, 3448, 18574, 1010, 13474, 2003, 2069, 27885, 14715, 3064, 2000, 5653, 2019, 16021, 12165, 2019, 3749, 2000, 20687, 4983, 13474, 2038, 2787, 2025, 2000, 20687, 1996, 3343, 1012, 13474, 3668, 2000, 16021, 12165, 2019, 3749, 2000, 20687, 1012, 16021, 12165, 2106, 2025, 5138, 1998, 1010, 3568, 1010, 1037, 2047, 3206, 1997, 5427, 2001, 2025, 2719, 1012, 2045, 2003, 2053, 2653, 1999, 1996, 3279, 3477, 3085, 11075, 2008, 5942, 13474, 2000, 2025, 8757, 20579, 1997, 16021, 12165, 1521, 1055, 4945, 2000, 20687, 1996, 3343, 1012, 1996, 2702, 1011, 2154, 5060, 9347, 21208, 2015, 2069, 16990, 1010, 2029, 2003, 1996, 18287, 1997, 6325, 2076, 1996, 3343, 2558, 1012, 2156, 13619, 25810, 1058, 1012, 5336, 16021, 1012, 2522, 1012, 1997, 2662, 1010, 4868, 12436, 1012, 25022, 2099, 1012, 2861, 1006, 2639, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 2008, 9347, 2515, 2025, 6611, 1999, 2023, 2553, 102, 3173, 2315, 4062, 15945, 15349, 14000, 6325, 2004, 2092, 2004, 8529, 6325, 2106, 2025, 24528, 8159, 2063, 8529, 11671, 2138, 11671, 3223, 8529, 6325, 2069, 2065, 1996, 4366, 4630, 4728, 24209, 11475, 14213, 2005, 14000, 6325, 2104, 1996, 3343, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1997, 5427, 1012, 2008, 11075, 4919, 2163, 2008, 1996, 13474, 2003, 27885, 14715, 3064, 2000, 2025, 8757, 20579, 1997, 1037, 1523, 16990, 1524, 1997, 1996, 3343, 1012, 16990, 2003, 2200, 2367, 2084, 4654, 16781, 1997, 1996, 3343, 1012, 2104, 1996, 3448, 18574, 1010, 13474, 2003, 2069, 27885, 14715, 3064, 2000, 5653, 2019, 16021, 12165, 2019, 3749, 2000, 20687, 4983, 13474, 2038, 2787, 2025, 2000, 20687, 1996, 3343, 1012, 13474, 3668, 2000, 16021, 12165, 2019, 3749, 2000, 20687, 1012, 16021, 12165, 2106, 2025, 5138, 1998, 1010, 3568, 1010, 1037, 2047, 3206, 1997, 5427, 2001, 2025, 2719, 1012, 2045, 2003, 2053, 2653, 1999, 1996, 3279, 3477, 3085, 11075, 2008, 5942, 13474, 2000, 2025, 8757, 20579, 1997, 16021, 12165, 1521, 1055, 4945, 2000, 20687, 1996, 3343, 1012, 1996, 2702, 1011, 2154, 5060, 9347, 21208, 2015, 2069, 16990, 1010, 2029, 2003, 1996, 18287, 1997, 6325, 2076, 1996, 3343, 2558, 1012, 2156, 13619, 25810, 1058, 1012, 5336, 16021, 1012, 2522, 1012, 1997, 2662, 1010, 4868, 12436, 1012, 25022, 2099, 1012, 2861, 1006, 2639, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 2008, 9347, 2515, 2025, 6611, 1999, 2023, 2553, 102, 3173, 2008, 2053, 6543, 11371, 2089, 5258, 2065, 1037, 24508, 2135, 5372, 2171, 14321, 22397, 4994, 2003, 3024, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1997, 5427, 1012, 2008, 11075, 4919, 2163, 2008, 1996, 13474, 2003, 27885, 14715, 3064, 2000, 2025, 8757, 20579, 1997, 1037, 1523, 16990, 1524, 1997, 1996, 3343, 1012, 16990, 2003, 2200, 2367, 2084, 4654, 16781, 1997, 1996, 3343, 1012, 2104, 1996, 3448, 18574, 1010, 13474, 2003, 2069, 27885, 14715, 3064, 2000, 5653, 2019, 16021, 12165, 2019, 3749, 2000, 20687, 4983, 13474, 2038, 2787, 2025, 2000, 20687, 1996, 3343, 1012, 13474, 3668, 2000, 16021, 12165, 2019, 3749, 2000, 20687, 1012, 16021, 12165, 2106, 2025, 5138, 1998, 1010, 3568, 1010, 1037, 2047, 3206, 1997, 5427, 2001, 2025, 2719, 1012, 2045, 2003, 2053, 2653, 1999, 1996, 3279, 3477, 3085, 11075, 2008, 5942, 13474, 2000, 2025, 8757, 20579, 1997, 16021, 12165, 1521, 1055, 4945, 2000, 20687, 1996, 3343, 1012, 1996, 2702, 1011, 2154, 5060, 9347, 21208, 2015, 2069, 16990, 1010, 2029, 2003, 1996, 18287, 1997, 6325, 2076, 1996, 3343, 2558, 1012, 2156, 13619, 25810, 1058, 1012, 5336, 16021, 1012, 2522, 1012, 1997, 2662, 1010, 4868, 12436, 1012, 25022, 2099, 1012, 2861, 1006, 2639, 1007, 1006, 1026, 3173, 1028, 1007, 1012, 2008, 9347, 2515, 2025, 6611, 1999, 2023, 2553, 102, 3173, 2008, 16990, 2089, 2069, 5258, 2096, 6325, 2003, 2145, 1999, 3466, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], attention_mask=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], token_type_ids=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], label=4).\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1606: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","[INFO|trainer.py:2121] 2023-06-08 23:42:56,643 >> Loading model from /content/gdrive/MyDrive/saved_model/bert_full_more_data.\n","[WARNING|trainer.py:2272] 2023-06-08 23:42:57,741 >> There were missing keys in the checkpoint model loaded: ['classifier.weight', 'classifier.bias'].\n","[WARNING|trainer.py:2274] 2023-06-08 23:42:57,741 >> There were unexpected keys in the checkpoint model loaded: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias'].\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1777] 2023-06-08 23:42:59,604 >> ***** Running training *****\n","[INFO|trainer.py:1778] 2023-06-08 23:42:59,605 >>   Num examples = 45,000\n","[INFO|trainer.py:1779] 2023-06-08 23:42:59,605 >>   Num Epochs = 20\n","[INFO|trainer.py:1780] 2023-06-08 23:42:59,605 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1781] 2023-06-08 23:42:59,605 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1782] 2023-06-08 23:42:59,605 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1783] 2023-06-08 23:42:59,605 >>   Total optimization steps = 112,500\n","[INFO|trainer.py:1784] 2023-06-08 23:42:59,605 >>   Number of trainable parameters = 109,483,009\n","  0% 0/112500 [00:00<?, ?it/s][INFO|trainer.py:2343] 2023-06-08 23:42:59,610 >> Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n","{'loss': 0.9988, 'learning_rate': 2.9867466666666668e-05, 'epoch': 0.09}\n","{'loss': 0.8463, 'learning_rate': 2.9734133333333334e-05, 'epoch': 0.18}\n","{'loss': 0.8235, 'learning_rate': 2.96008e-05, 'epoch': 0.27}\n","{'loss': 0.8194, 'learning_rate': 2.9467466666666665e-05, 'epoch': 0.36}\n","{'loss': 0.7893, 'learning_rate': 2.9334133333333334e-05, 'epoch': 0.44}\n","{'loss': 0.7913, 'learning_rate': 2.92008e-05, 'epoch': 0.53}\n","{'loss': 0.7918, 'learning_rate': 2.9067466666666666e-05, 'epoch': 0.62}\n","{'loss': 0.7637, 'learning_rate': 2.893413333333333e-05, 'epoch': 0.71}\n","{'loss': 0.7605, 'learning_rate': 2.8801066666666668e-05, 'epoch': 0.8}\n","{'loss': 0.7668, 'learning_rate': 2.8668e-05, 'epoch': 0.89}\n","{'loss': 0.785, 'learning_rate': 2.8534666666666666e-05, 'epoch': 0.98}\n","  5% 5625/112500 [10:18<3:15:07,  9.13it/s][INFO|trainer.py:3190] 2023-06-08 23:53:18,478 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3192] 2023-06-08 23:53:18,478 >>   Num examples = 3900\n","[INFO|trainer.py:3195] 2023-06-08 23:53:18,478 >>   Batch size = 8\n","\n","  0% 0/488 [00:00<?, ?it/s]\u001b[A\n","  1% 4/488 [00:00<00:12, 39.52it/s]\u001b[A\n","  2% 8/488 [00:00<00:14, 33.12it/s]\u001b[A\n","  2% 12/488 [00:00<00:15, 31.52it/s]\u001b[A\n","  3% 16/488 [00:00<00:15, 30.76it/s]\u001b[A\n","  4% 20/488 [00:00<00:15, 30.39it/s]\u001b[A\n","  5% 24/488 [00:00<00:15, 30.20it/s]\u001b[A\n","  6% 28/488 [00:00<00:15, 30.07it/s]\u001b[A\n","  7% 32/488 [00:01<00:15, 29.98it/s]\u001b[A\n","  7% 36/488 [00:01<00:15, 29.86it/s]\u001b[A\n","  8% 39/488 [00:01<00:15, 29.85it/s]\u001b[A\n","  9% 42/488 [00:01<00:14, 29.83it/s]\u001b[A\n","  9% 45/488 [00:01<00:14, 29.83it/s]\u001b[A\n"," 10% 48/488 [00:01<00:14, 29.81it/s]\u001b[A\n"," 10% 51/488 [00:01<00:14, 29.81it/s]\u001b[A\n"," 11% 54/488 [00:01<00:14, 29.82it/s]\u001b[A\n"," 12% 57/488 [00:01<00:14, 29.80it/s]\u001b[A\n"," 12% 60/488 [00:01<00:14, 29.82it/s]\u001b[A\n"," 13% 63/488 [00:02<00:14, 29.81it/s]\u001b[A\n"," 14% 66/488 [00:02<00:14, 29.81it/s]\u001b[A\n"," 14% 69/488 [00:02<00:14, 29.82it/s]\u001b[A\n"," 15% 72/488 [00:02<00:13, 29.84it/s]\u001b[A\n"," 15% 75/488 [00:02<00:13, 29.84it/s]\u001b[A\n"," 16% 78/488 [00:02<00:13, 29.81it/s]\u001b[A\n"," 17% 81/488 [00:02<00:13, 29.81it/s]\u001b[A\n"," 17% 84/488 [00:02<00:13, 29.82it/s]\u001b[A\n"," 18% 87/488 [00:02<00:13, 29.81it/s]\u001b[A\n"," 18% 90/488 [00:02<00:13, 29.82it/s]\u001b[A\n"," 19% 93/488 [00:03<00:13, 29.83it/s]\u001b[A\n"," 20% 96/488 [00:03<00:13, 29.83it/s]\u001b[A\n"," 20% 99/488 [00:03<00:13, 29.83it/s]\u001b[A\n"," 21% 102/488 [00:03<00:12, 29.82it/s]\u001b[A\n"," 22% 105/488 [00:03<00:12, 29.82it/s]\u001b[A\n"," 22% 108/488 [00:03<00:12, 29.80it/s]\u001b[A\n"," 23% 111/488 [00:03<00:12, 29.80it/s]\u001b[A\n"," 23% 114/488 [00:03<00:12, 29.77it/s]\u001b[A\n"," 24% 117/488 [00:03<00:12, 29.77it/s]\u001b[A\n"," 25% 120/488 [00:03<00:12, 29.79it/s]\u001b[A\n"," 25% 123/488 [00:04<00:12, 29.80it/s]\u001b[A\n"," 26% 126/488 [00:04<00:12, 29.79it/s]\u001b[A\n"," 26% 129/488 [00:04<00:12, 29.80it/s]\u001b[A\n"," 27% 132/488 [00:04<00:11, 29.81it/s]\u001b[A\n"," 28% 135/488 [00:04<00:11, 29.82it/s]\u001b[A\n"," 28% 138/488 [00:04<00:11, 29.80it/s]\u001b[A\n"," 29% 141/488 [00:04<00:11, 29.77it/s]\u001b[A\n"," 30% 144/488 [00:04<00:11, 29.79it/s]\u001b[A\n"," 30% 147/488 [00:04<00:11, 29.80it/s]\u001b[A\n"," 31% 150/488 [00:05<00:11, 29.67it/s]\u001b[A\n"," 31% 153/488 [00:05<00:11, 29.72it/s]\u001b[A\n"," 32% 156/488 [00:05<00:11, 29.76it/s]\u001b[A\n"," 33% 159/488 [00:05<00:11, 29.75it/s]\u001b[A\n"," 33% 162/488 [00:05<00:10, 29.77it/s]\u001b[A\n"," 34% 165/488 [00:05<00:10, 29.78it/s]\u001b[A\n"," 34% 168/488 [00:05<00:10, 29.59it/s]\u001b[A\n"," 35% 171/488 [00:05<00:10, 29.65it/s]\u001b[A\n"," 36% 174/488 [00:05<00:10, 29.68it/s]\u001b[A\n"," 36% 177/488 [00:05<00:10, 29.71it/s]\u001b[A\n"," 37% 180/488 [00:06<00:10, 29.73it/s]\u001b[A\n"," 38% 183/488 [00:06<00:10, 29.75it/s]\u001b[A\n"," 38% 186/488 [00:06<00:10, 29.74it/s]\u001b[A\n"," 39% 189/488 [00:06<00:10, 29.76it/s]\u001b[A\n"," 39% 192/488 [00:06<00:09, 29.77it/s]\u001b[A\n"," 40% 195/488 [00:06<00:09, 29.79it/s]\u001b[A\n"," 41% 198/488 [00:06<00:09, 29.77it/s]\u001b[A\n"," 41% 201/488 [00:06<00:09, 29.76it/s]\u001b[A\n"," 42% 204/488 [00:06<00:09, 29.78it/s]\u001b[A\n"," 42% 207/488 [00:06<00:09, 29.78it/s]\u001b[A\n"," 43% 210/488 [00:07<00:09, 29.80it/s]\u001b[A\n"," 44% 213/488 [00:07<00:09, 29.81it/s]\u001b[A\n"," 44% 216/488 [00:07<00:09, 29.81it/s]\u001b[A\n"," 45% 219/488 [00:07<00:09, 29.82it/s]\u001b[A\n"," 45% 222/488 [00:07<00:08, 29.82it/s]\u001b[A\n"," 46% 225/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 47% 228/488 [00:07<00:08, 29.76it/s]\u001b[A\n"," 47% 231/488 [00:07<00:08, 29.55it/s]\u001b[A\n"," 48% 234/488 [00:07<00:08, 29.64it/s]\u001b[A\n"," 49% 237/488 [00:07<00:08, 29.71it/s]\u001b[A\n"," 49% 240/488 [00:08<00:08, 29.75it/s]\u001b[A\n"," 50% 243/488 [00:08<00:08, 29.79it/s]\u001b[A\n"," 50% 246/488 [00:08<00:08, 29.81it/s]\u001b[A\n"," 51% 249/488 [00:08<00:08, 29.76it/s]\u001b[A\n"," 52% 252/488 [00:08<00:07, 29.76it/s]\u001b[A\n"," 52% 255/488 [00:08<00:07, 29.77it/s]\u001b[A\n"," 53% 258/488 [00:08<00:07, 29.72it/s]\u001b[A\n"," 53% 261/488 [00:08<00:07, 29.74it/s]\u001b[A\n"," 54% 264/488 [00:08<00:07, 29.76it/s]\u001b[A\n"," 55% 267/488 [00:08<00:07, 29.77it/s]\u001b[A\n"," 55% 270/488 [00:09<00:07, 29.78it/s]\u001b[A\n"," 56% 273/488 [00:09<00:07, 29.79it/s]\u001b[A\n"," 57% 276/488 [00:09<00:07, 29.77it/s]\u001b[A\n"," 57% 279/488 [00:09<00:07, 29.75it/s]\u001b[A\n"," 58% 282/488 [00:09<00:06, 29.76it/s]\u001b[A\n"," 58% 285/488 [00:09<00:06, 29.78it/s]\u001b[A\n"," 59% 288/488 [00:09<00:06, 29.79it/s]\u001b[A\n"," 60% 291/488 [00:09<00:06, 29.53it/s]\u001b[A\n"," 60% 294/488 [00:09<00:06, 29.61it/s]\u001b[A\n"," 61% 297/488 [00:09<00:06, 29.67it/s]\u001b[A\n"," 61% 300/488 [00:10<00:06, 29.70it/s]\u001b[A\n"," 62% 303/488 [00:10<00:06, 29.74it/s]\u001b[A\n"," 63% 306/488 [00:10<00:06, 29.77it/s]\u001b[A\n"," 63% 309/488 [00:10<00:06, 29.77it/s]\u001b[A\n"," 64% 312/488 [00:10<00:05, 29.80it/s]\u001b[A\n"," 65% 315/488 [00:10<00:05, 29.80it/s]\u001b[A\n"," 65% 318/488 [00:10<00:05, 29.78it/s]\u001b[A\n"," 66% 321/488 [00:10<00:05, 29.75it/s]\u001b[A\n"," 66% 324/488 [00:10<00:05, 29.74it/s]\u001b[A\n"," 67% 327/488 [00:10<00:05, 29.71it/s]\u001b[A\n"," 68% 330/488 [00:11<00:05, 29.72it/s]\u001b[A\n"," 68% 333/488 [00:11<00:05, 29.70it/s]\u001b[A\n"," 69% 336/488 [00:11<00:05, 29.71it/s]\u001b[A\n"," 69% 339/488 [00:11<00:05, 29.69it/s]\u001b[A\n"," 70% 342/488 [00:11<00:04, 29.70it/s]\u001b[A\n"," 71% 345/488 [00:11<00:04, 29.70it/s]\u001b[A\n"," 71% 348/488 [00:11<00:04, 29.70it/s]\u001b[A\n"," 72% 351/488 [00:11<00:04, 29.70it/s]\u001b[A\n"," 73% 354/488 [00:11<00:04, 29.71it/s]\u001b[A\n"," 73% 357/488 [00:11<00:04, 29.73it/s]\u001b[A\n"," 74% 360/488 [00:12<00:04, 29.76it/s]\u001b[A\n"," 74% 363/488 [00:12<00:04, 29.78it/s]\u001b[A\n"," 75% 366/488 [00:12<00:04, 29.80it/s]\u001b[A\n"," 76% 369/488 [00:12<00:03, 29.80it/s]\u001b[A\n"," 76% 372/488 [00:12<00:03, 29.81it/s]\u001b[A\n"," 77% 375/488 [00:12<00:03, 29.76it/s]\u001b[A\n"," 77% 378/488 [00:12<00:03, 29.75it/s]\u001b[A\n"," 78% 381/488 [00:12<00:03, 29.77it/s]\u001b[A\n"," 79% 384/488 [00:12<00:03, 29.77it/s]\u001b[A\n"," 79% 387/488 [00:12<00:03, 29.77it/s]\u001b[A\n"," 80% 390/488 [00:13<00:03, 29.78it/s]\u001b[A\n"," 81% 393/488 [00:13<00:03, 29.80it/s]\u001b[A\n"," 81% 396/488 [00:13<00:03, 29.79it/s]\u001b[A\n"," 82% 399/488 [00:13<00:02, 29.80it/s]\u001b[A\n"," 82% 402/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 83% 405/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 84% 408/488 [00:13<00:02, 29.80it/s]\u001b[A\n"," 84% 411/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 85% 414/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 85% 417/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 86% 420/488 [00:14<00:02, 29.80it/s]\u001b[A\n"," 87% 423/488 [00:14<00:02, 29.79it/s]\u001b[A\n"," 87% 426/488 [00:14<00:02, 29.79it/s]\u001b[A\n"," 88% 429/488 [00:14<00:01, 29.78it/s]\u001b[A\n"," 89% 432/488 [00:14<00:01, 29.80it/s]\u001b[A\n"," 89% 435/488 [00:14<00:01, 29.81it/s]\u001b[A\n"," 90% 438/488 [00:14<00:01, 29.80it/s]\u001b[A\n"," 90% 441/488 [00:14<00:01, 29.79it/s]\u001b[A\n"," 91% 444/488 [00:14<00:01, 29.77it/s]\u001b[A\n"," 92% 447/488 [00:14<00:01, 29.79it/s]\u001b[A\n"," 92% 450/488 [00:15<00:01, 29.81it/s]\u001b[A\n"," 93% 453/488 [00:15<00:01, 29.82it/s]\u001b[A\n"," 93% 456/488 [00:15<00:01, 29.83it/s]\u001b[A\n"," 94% 459/488 [00:15<00:00, 29.83it/s]\u001b[A\n"," 95% 462/488 [00:15<00:00, 29.83it/s]\u001b[A\n"," 95% 465/488 [00:15<00:00, 29.83it/s]\u001b[A\n"," 96% 468/488 [00:15<00:00, 29.77it/s]\u001b[A\n"," 97% 471/488 [00:15<00:00, 29.78it/s]\u001b[A\n"," 97% 474/488 [00:15<00:00, 29.71it/s]\u001b[A\n"," 98% 477/488 [00:15<00:00, 29.69it/s]\u001b[A\n"," 98% 480/488 [00:16<00:00, 29.70it/s]\u001b[A\n"," 99% 483/488 [00:16<00:00, 29.73it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 0.6927909851074219, 'eval_macro-f1': 0.7212178104611366, 'eval_micro-f1': 0.7212820512820513, 'eval_runtime': 16.3948, 'eval_samples_per_second': 237.88, 'eval_steps_per_second': 29.765, 'epoch': 1.0}\n","  5% 5625/112500 [10:35<3:15:07,  9.13it/s]\n","100% 488/488 [00:16<00:00, 29.75it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:2916] 2023-06-08 23:53:34,874 >> Saving model checkpoint to logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-5625\n","[INFO|configuration_utils.py:458] 2023-06-08 23:53:34,875 >> Configuration saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-5625/config.json\n","[INFO|modeling_utils.py:1853] 2023-06-08 23:53:35,429 >> Model weights saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-5625/pytorch_model.bin\n","{'loss': 0.6019, 'learning_rate': 2.8401333333333332e-05, 'epoch': 1.07}\n","{'loss': 0.5779, 'learning_rate': 2.8268e-05, 'epoch': 1.16}\n","{'loss': 0.5981, 'learning_rate': 2.8134933333333334e-05, 'epoch': 1.24}\n","{'loss': 0.5865, 'learning_rate': 2.8001600000000003e-05, 'epoch': 1.33}\n","{'loss': 0.5827, 'learning_rate': 2.786826666666667e-05, 'epoch': 1.42}\n","{'loss': 0.6102, 'learning_rate': 2.7734933333333334e-05, 'epoch': 1.51}\n","{'loss': 0.6074, 'learning_rate': 2.76016e-05, 'epoch': 1.6}\n","{'loss': 0.6289, 'learning_rate': 2.74688e-05, 'epoch': 1.69}\n","{'loss': 0.6147, 'learning_rate': 2.7335466666666666e-05, 'epoch': 1.78}\n","{'loss': 0.6077, 'learning_rate': 2.7202133333333332e-05, 'epoch': 1.87}\n","{'loss': 0.6341, 'learning_rate': 2.70688e-05, 'epoch': 1.96}\n"," 10% 11250/112500 [20:53<3:07:23,  9.01it/s][INFO|trainer.py:3190] 2023-06-09 00:03:53,349 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3192] 2023-06-09 00:03:53,349 >>   Num examples = 3900\n","[INFO|trainer.py:3195] 2023-06-09 00:03:53,349 >>   Batch size = 8\n","\n","  0% 0/488 [00:00<?, ?it/s]\u001b[A\n","  1% 4/488 [00:00<00:12, 39.49it/s]\u001b[A\n","  2% 8/488 [00:00<00:14, 33.07it/s]\u001b[A\n","  2% 12/488 [00:00<00:15, 31.43it/s]\u001b[A\n","  3% 16/488 [00:00<00:15, 30.69it/s]\u001b[A\n","  4% 20/488 [00:00<00:15, 30.25it/s]\u001b[A\n","  5% 24/488 [00:00<00:15, 29.97it/s]\u001b[A\n","  6% 28/488 [00:00<00:15, 29.81it/s]\u001b[A\n","  6% 31/488 [00:01<00:15, 29.73it/s]\u001b[A\n","  7% 34/488 [00:01<00:15, 29.72it/s]\u001b[A\n","  8% 37/488 [00:01<00:15, 29.69it/s]\u001b[A\n","  8% 40/488 [00:01<00:15, 29.28it/s]\u001b[A\n","  9% 43/488 [00:01<00:15, 29.37it/s]\u001b[A\n","  9% 46/488 [00:01<00:15, 29.46it/s]\u001b[A\n"," 10% 49/488 [00:01<00:14, 29.54it/s]\u001b[A\n"," 11% 52/488 [00:01<00:14, 29.57it/s]\u001b[A\n"," 11% 55/488 [00:01<00:14, 29.65it/s]\u001b[A\n"," 12% 58/488 [00:01<00:14, 29.69it/s]\u001b[A\n"," 12% 61/488 [00:02<00:14, 29.74it/s]\u001b[A\n"," 13% 64/488 [00:02<00:14, 29.78it/s]\u001b[A\n"," 14% 67/488 [00:02<00:14, 29.79it/s]\u001b[A\n"," 14% 70/488 [00:02<00:14, 29.82it/s]\u001b[A\n"," 15% 73/488 [00:02<00:13, 29.84it/s]\u001b[A\n"," 16% 76/488 [00:02<00:13, 29.83it/s]\u001b[A\n"," 16% 79/488 [00:02<00:13, 29.84it/s]\u001b[A\n"," 17% 82/488 [00:02<00:13, 29.85it/s]\u001b[A\n"," 17% 85/488 [00:02<00:13, 29.84it/s]\u001b[A\n"," 18% 88/488 [00:02<00:13, 29.84it/s]\u001b[A\n"," 19% 91/488 [00:03<00:13, 29.82it/s]\u001b[A\n"," 19% 94/488 [00:03<00:13, 29.82it/s]\u001b[A\n"," 20% 97/488 [00:03<00:13, 29.82it/s]\u001b[A\n"," 20% 100/488 [00:03<00:13, 29.82it/s]\u001b[A\n"," 21% 103/488 [00:03<00:12, 29.83it/s]\u001b[A\n"," 22% 106/488 [00:03<00:12, 29.83it/s]\u001b[A\n"," 22% 109/488 [00:03<00:12, 29.81it/s]\u001b[A\n"," 23% 112/488 [00:03<00:12, 29.82it/s]\u001b[A\n"," 24% 115/488 [00:03<00:12, 29.81it/s]\u001b[A\n"," 24% 118/488 [00:03<00:12, 29.79it/s]\u001b[A\n"," 25% 121/488 [00:04<00:12, 29.71it/s]\u001b[A\n"," 25% 124/488 [00:04<00:12, 29.74it/s]\u001b[A\n"," 26% 127/488 [00:04<00:12, 29.75it/s]\u001b[A\n"," 27% 130/488 [00:04<00:12, 29.74it/s]\u001b[A\n"," 27% 133/488 [00:04<00:11, 29.75it/s]\u001b[A\n"," 28% 136/488 [00:04<00:11, 29.75it/s]\u001b[A\n"," 28% 139/488 [00:04<00:11, 29.75it/s]\u001b[A\n"," 29% 142/488 [00:04<00:11, 29.77it/s]\u001b[A\n"," 30% 145/488 [00:04<00:11, 29.78it/s]\u001b[A\n"," 30% 148/488 [00:04<00:11, 29.79it/s]\u001b[A\n"," 31% 151/488 [00:05<00:11, 29.80it/s]\u001b[A\n"," 32% 154/488 [00:05<00:11, 29.81it/s]\u001b[A\n"," 32% 157/488 [00:05<00:11, 29.81it/s]\u001b[A\n"," 33% 160/488 [00:05<00:11, 29.81it/s]\u001b[A\n"," 33% 163/488 [00:05<00:10, 29.82it/s]\u001b[A\n"," 34% 166/488 [00:05<00:10, 29.82it/s]\u001b[A\n"," 35% 169/488 [00:05<00:10, 29.81it/s]\u001b[A\n"," 35% 172/488 [00:05<00:10, 29.80it/s]\u001b[A\n"," 36% 175/488 [00:05<00:10, 29.80it/s]\u001b[A\n"," 36% 178/488 [00:05<00:10, 29.81it/s]\u001b[A\n"," 37% 181/488 [00:06<00:10, 29.81it/s]\u001b[A\n"," 38% 184/488 [00:06<00:10, 29.82it/s]\u001b[A\n"," 38% 187/488 [00:06<00:10, 29.81it/s]\u001b[A\n"," 39% 190/488 [00:06<00:09, 29.80it/s]\u001b[A\n"," 40% 193/488 [00:06<00:09, 29.79it/s]\u001b[A\n"," 40% 196/488 [00:06<00:09, 29.78it/s]\u001b[A\n"," 41% 199/488 [00:06<00:09, 29.79it/s]\u001b[A\n"," 41% 202/488 [00:06<00:09, 29.77it/s]\u001b[A\n"," 42% 205/488 [00:06<00:09, 29.79it/s]\u001b[A\n"," 43% 208/488 [00:06<00:09, 29.80it/s]\u001b[A\n"," 43% 211/488 [00:07<00:09, 29.76it/s]\u001b[A\n"," 44% 214/488 [00:07<00:09, 29.79it/s]\u001b[A\n"," 44% 217/488 [00:07<00:09, 29.80it/s]\u001b[A\n"," 45% 220/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 46% 223/488 [00:07<00:08, 29.82it/s]\u001b[A\n"," 46% 226/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 47% 229/488 [00:07<00:08, 29.56it/s]\u001b[A\n"," 48% 232/488 [00:07<00:08, 29.65it/s]\u001b[A\n"," 48% 235/488 [00:07<00:08, 29.69it/s]\u001b[A\n"," 49% 238/488 [00:07<00:08, 29.73it/s]\u001b[A\n"," 49% 241/488 [00:08<00:08, 29.75it/s]\u001b[A\n"," 50% 244/488 [00:08<00:08, 29.77it/s]\u001b[A\n"," 51% 247/488 [00:08<00:08, 29.79it/s]\u001b[A\n"," 51% 250/488 [00:08<00:07, 29.77it/s]\u001b[A\n"," 52% 253/488 [00:08<00:07, 29.65it/s]\u001b[A\n"," 52% 256/488 [00:08<00:07, 29.68it/s]\u001b[A\n"," 53% 259/488 [00:08<00:07, 29.72it/s]\u001b[A\n"," 54% 262/488 [00:08<00:07, 29.75it/s]\u001b[A\n"," 54% 265/488 [00:08<00:07, 29.76it/s]\u001b[A\n"," 55% 268/488 [00:08<00:07, 29.78it/s]\u001b[A\n"," 56% 271/488 [00:09<00:07, 29.80it/s]\u001b[A\n"," 56% 274/488 [00:09<00:07, 29.81it/s]\u001b[A\n"," 57% 277/488 [00:09<00:07, 29.82it/s]\u001b[A\n"," 57% 280/488 [00:09<00:06, 29.82it/s]\u001b[A\n"," 58% 283/488 [00:09<00:06, 29.84it/s]\u001b[A\n"," 59% 286/488 [00:09<00:06, 29.84it/s]\u001b[A\n"," 59% 289/488 [00:09<00:06, 29.84it/s]\u001b[A\n"," 60% 292/488 [00:09<00:06, 29.81it/s]\u001b[A\n"," 60% 295/488 [00:09<00:06, 29.80it/s]\u001b[A\n"," 61% 298/488 [00:09<00:06, 29.81it/s]\u001b[A\n"," 62% 301/488 [00:10<00:06, 29.82it/s]\u001b[A\n"," 62% 304/488 [00:10<00:06, 29.84it/s]\u001b[A\n"," 63% 307/488 [00:10<00:06, 29.83it/s]\u001b[A\n"," 64% 310/488 [00:10<00:05, 29.82it/s]\u001b[A\n"," 64% 313/488 [00:10<00:05, 29.84it/s]\u001b[A\n"," 65% 316/488 [00:10<00:05, 29.84it/s]\u001b[A\n"," 65% 319/488 [00:10<00:05, 29.83it/s]\u001b[A\n"," 66% 322/488 [00:10<00:05, 29.83it/s]\u001b[A\n"," 67% 325/488 [00:10<00:05, 29.83it/s]\u001b[A\n"," 67% 328/488 [00:10<00:05, 29.84it/s]\u001b[A\n"," 68% 331/488 [00:11<00:05, 29.83it/s]\u001b[A\n"," 68% 334/488 [00:11<00:05, 29.84it/s]\u001b[A\n"," 69% 337/488 [00:11<00:05, 29.84it/s]\u001b[A\n"," 70% 340/488 [00:11<00:04, 29.82it/s]\u001b[A\n"," 70% 343/488 [00:11<00:04, 29.76it/s]\u001b[A\n"," 71% 346/488 [00:11<00:04, 29.77it/s]\u001b[A\n"," 72% 349/488 [00:11<00:04, 29.77it/s]\u001b[A\n"," 72% 352/488 [00:11<00:04, 29.78it/s]\u001b[A\n"," 73% 355/488 [00:11<00:04, 29.77it/s]\u001b[A\n"," 73% 358/488 [00:11<00:04, 29.78it/s]\u001b[A\n"," 74% 361/488 [00:12<00:04, 29.80it/s]\u001b[A\n"," 75% 364/488 [00:12<00:04, 29.82it/s]\u001b[A\n"," 75% 367/488 [00:12<00:04, 29.83it/s]\u001b[A\n"," 76% 370/488 [00:12<00:03, 29.83it/s]\u001b[A\n"," 76% 373/488 [00:12<00:03, 29.84it/s]\u001b[A\n"," 77% 376/488 [00:12<00:03, 29.84it/s]\u001b[A\n"," 78% 379/488 [00:12<00:03, 29.86it/s]\u001b[A\n"," 78% 382/488 [00:12<00:03, 29.85it/s]\u001b[A\n"," 79% 385/488 [00:12<00:03, 29.83it/s]\u001b[A\n"," 80% 388/488 [00:13<00:03, 29.79it/s]\u001b[A\n"," 80% 391/488 [00:13<00:03, 29.79it/s]\u001b[A\n"," 81% 394/488 [00:13<00:03, 29.78it/s]\u001b[A\n"," 81% 397/488 [00:13<00:03, 29.80it/s]\u001b[A\n"," 82% 400/488 [00:13<00:02, 29.80it/s]\u001b[A\n"," 83% 403/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 83% 406/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 84% 409/488 [00:13<00:02, 29.77it/s]\u001b[A\n"," 84% 412/488 [00:13<00:02, 29.72it/s]\u001b[A\n"," 85% 415/488 [00:13<00:02, 29.73it/s]\u001b[A\n"," 86% 418/488 [00:14<00:02, 29.75it/s]\u001b[A\n"," 86% 421/488 [00:14<00:02, 29.76it/s]\u001b[A\n"," 87% 424/488 [00:14<00:02, 29.77it/s]\u001b[A\n"," 88% 427/488 [00:14<00:02, 29.77it/s]\u001b[A\n"," 88% 430/488 [00:14<00:01, 29.76it/s]\u001b[A\n"," 89% 433/488 [00:14<00:01, 29.77it/s]\u001b[A\n"," 89% 436/488 [00:14<00:01, 29.77it/s]\u001b[A\n"," 90% 439/488 [00:14<00:01, 29.69it/s]\u001b[A\n"," 91% 442/488 [00:14<00:01, 29.73it/s]\u001b[A\n"," 91% 445/488 [00:14<00:01, 29.75it/s]\u001b[A\n"," 92% 448/488 [00:15<00:01, 29.75it/s]\u001b[A\n"," 92% 451/488 [00:15<00:01, 29.78it/s]\u001b[A\n"," 93% 454/488 [00:15<00:01, 29.81it/s]\u001b[A\n"," 94% 457/488 [00:15<00:01, 29.83it/s]\u001b[A\n"," 94% 460/488 [00:15<00:00, 29.84it/s]\u001b[A\n"," 95% 463/488 [00:15<00:00, 29.80it/s]\u001b[A\n"," 95% 466/488 [00:15<00:00, 29.82it/s]\u001b[A\n"," 96% 469/488 [00:15<00:00, 29.81it/s]\u001b[A\n"," 97% 472/488 [00:15<00:00, 29.82it/s]\u001b[A\n"," 97% 475/488 [00:15<00:00, 29.83it/s]\u001b[A\n"," 98% 478/488 [00:16<00:00, 29.82it/s]\u001b[A\n"," 99% 481/488 [00:16<00:00, 29.83it/s]\u001b[A\n"," 99% 484/488 [00:16<00:00, 29.84it/s]\u001b[A\n","                                            \n","\u001b[A{'eval_loss': 0.7821032404899597, 'eval_macro-f1': 0.7224374185127277, 'eval_micro-f1': 0.7225641025641026, 'eval_runtime': 16.3886, 'eval_samples_per_second': 237.971, 'eval_steps_per_second': 29.777, 'epoch': 2.0}\n"," 10% 11250/112500 [21:10<3:07:23,  9.01it/s]\n","100% 488/488 [00:16<00:00, 29.85it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:2916] 2023-06-09 00:04:09,739 >> Saving model checkpoint to logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-11250\n","[INFO|configuration_utils.py:458] 2023-06-09 00:04:09,739 >> Configuration saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-11250/config.json\n","[INFO|modeling_utils.py:1853] 2023-06-09 00:04:10,291 >> Model weights saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-11250/pytorch_model.bin\n","{'loss': 0.5007, 'learning_rate': 2.693546666666667e-05, 'epoch': 2.04}\n","{'loss': 0.3935, 'learning_rate': 2.68024e-05, 'epoch': 2.13}\n","{'loss': 0.4062, 'learning_rate': 2.666906666666667e-05, 'epoch': 2.22}\n","{'loss': 0.4012, 'learning_rate': 2.6535733333333334e-05, 'epoch': 2.31}\n","{'loss': 0.4328, 'learning_rate': 2.64024e-05, 'epoch': 2.4}\n","{'loss': 0.4386, 'learning_rate': 2.6269066666666666e-05, 'epoch': 2.49}\n","{'loss': 0.4474, 'learning_rate': 2.6135733333333335e-05, 'epoch': 2.58}\n","{'loss': 0.4399, 'learning_rate': 2.60024e-05, 'epoch': 2.67}\n","{'loss': 0.446, 'learning_rate': 2.5869066666666666e-05, 'epoch': 2.76}\n","{'loss': 0.4438, 'learning_rate': 2.5735733333333335e-05, 'epoch': 2.84}\n","{'loss': 0.4547, 'learning_rate': 2.5602666666666668e-05, 'epoch': 2.93}\n"," 15% 16875/112500 [31:27<2:53:27,  9.19it/s][INFO|trainer.py:3190] 2023-06-09 00:14:27,346 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3192] 2023-06-09 00:14:27,346 >>   Num examples = 3900\n","[INFO|trainer.py:3195] 2023-06-09 00:14:27,346 >>   Batch size = 8\n","\n","  0% 0/488 [00:00<?, ?it/s]\u001b[A\n","  1% 4/488 [00:00<00:12, 39.81it/s]\u001b[A\n","  2% 8/488 [00:00<00:14, 33.25it/s]\u001b[A\n","  2% 12/488 [00:00<00:15, 31.28it/s]\u001b[A\n","  3% 16/488 [00:00<00:15, 30.66it/s]\u001b[A\n","  4% 20/488 [00:00<00:15, 29.95it/s]\u001b[A\n","  5% 24/488 [00:00<00:15, 29.91it/s]\u001b[A\n","  6% 28/488 [00:00<00:15, 29.87it/s]\u001b[A\n","  6% 31/488 [00:01<00:15, 29.86it/s]\u001b[A\n","  7% 34/488 [00:01<00:15, 29.85it/s]\u001b[A\n","  8% 37/488 [00:01<00:15, 29.84it/s]\u001b[A\n","  8% 40/488 [00:01<00:15, 29.82it/s]\u001b[A\n","  9% 43/488 [00:01<00:14, 29.83it/s]\u001b[A\n","  9% 46/488 [00:01<00:14, 29.83it/s]\u001b[A\n"," 10% 49/488 [00:01<00:14, 29.82it/s]\u001b[A\n"," 11% 52/488 [00:01<00:14, 29.41it/s]\u001b[A\n"," 11% 55/488 [00:01<00:14, 29.49it/s]\u001b[A\n"," 12% 58/488 [00:01<00:14, 29.55it/s]\u001b[A\n"," 12% 61/488 [00:02<00:14, 29.60it/s]\u001b[A\n"," 13% 64/488 [00:02<00:14, 29.62it/s]\u001b[A\n"," 14% 67/488 [00:02<00:14, 29.52it/s]\u001b[A\n"," 14% 70/488 [00:02<00:14, 29.58it/s]\u001b[A\n"," 15% 73/488 [00:02<00:14, 29.61it/s]\u001b[A\n"," 16% 76/488 [00:02<00:13, 29.65it/s]\u001b[A\n"," 16% 79/488 [00:02<00:13, 29.68it/s]\u001b[A\n"," 17% 82/488 [00:02<00:13, 29.68it/s]\u001b[A\n"," 17% 85/488 [00:02<00:13, 29.68it/s]\u001b[A\n"," 18% 88/488 [00:02<00:13, 29.68it/s]\u001b[A\n"," 19% 91/488 [00:03<00:13, 29.69it/s]\u001b[A\n"," 19% 94/488 [00:03<00:13, 29.69it/s]\u001b[A\n"," 20% 97/488 [00:03<00:13, 29.67it/s]\u001b[A\n"," 20% 100/488 [00:03<00:13, 29.67it/s]\u001b[A\n"," 21% 103/488 [00:03<00:13, 29.45it/s]\u001b[A\n"," 22% 106/488 [00:03<00:12, 29.52it/s]\u001b[A\n"," 22% 109/488 [00:03<00:12, 29.58it/s]\u001b[A\n"," 23% 112/488 [00:03<00:12, 29.61it/s]\u001b[A\n"," 24% 115/488 [00:03<00:12, 29.61it/s]\u001b[A\n"," 24% 118/488 [00:03<00:12, 29.64it/s]\u001b[A\n"," 25% 121/488 [00:04<00:12, 29.66it/s]\u001b[A\n"," 25% 124/488 [00:04<00:12, 29.68it/s]\u001b[A\n"," 26% 127/488 [00:04<00:12, 29.69it/s]\u001b[A\n"," 27% 130/488 [00:04<00:12, 29.70it/s]\u001b[A\n"," 27% 133/488 [00:04<00:11, 29.68it/s]\u001b[A\n"," 28% 136/488 [00:04<00:11, 29.69it/s]\u001b[A\n"," 28% 139/488 [00:04<00:11, 29.70it/s]\u001b[A\n"," 29% 142/488 [00:04<00:11, 29.70it/s]\u001b[A\n"," 30% 145/488 [00:04<00:11, 29.71it/s]\u001b[A\n"," 30% 148/488 [00:04<00:11, 29.71it/s]\u001b[A\n"," 31% 151/488 [00:05<00:11, 29.71it/s]\u001b[A\n"," 32% 154/488 [00:05<00:11, 29.69it/s]\u001b[A\n"," 32% 157/488 [00:05<00:11, 29.71it/s]\u001b[A\n"," 33% 160/488 [00:05<00:11, 29.71it/s]\u001b[A\n"," 33% 163/488 [00:05<00:10, 29.73it/s]\u001b[A\n"," 34% 166/488 [00:05<00:10, 29.73it/s]\u001b[A\n"," 35% 169/488 [00:05<00:10, 29.75it/s]\u001b[A\n"," 35% 172/488 [00:05<00:10, 29.75it/s]\u001b[A\n"," 36% 175/488 [00:05<00:10, 29.73it/s]\u001b[A\n"," 36% 178/488 [00:05<00:10, 29.75it/s]\u001b[A\n"," 37% 181/488 [00:06<00:10, 29.76it/s]\u001b[A\n"," 38% 184/488 [00:06<00:10, 29.76it/s]\u001b[A\n"," 38% 187/488 [00:06<00:10, 29.77it/s]\u001b[A\n"," 39% 190/488 [00:06<00:10, 29.78it/s]\u001b[A\n"," 40% 193/488 [00:06<00:09, 29.79it/s]\u001b[A\n"," 40% 196/488 [00:06<00:09, 29.79it/s]\u001b[A\n"," 41% 199/488 [00:06<00:09, 29.80it/s]\u001b[A\n"," 41% 202/488 [00:06<00:09, 29.81it/s]\u001b[A\n"," 42% 205/488 [00:06<00:09, 29.80it/s]\u001b[A\n"," 43% 208/488 [00:06<00:09, 29.81it/s]\u001b[A\n"," 43% 211/488 [00:07<00:09, 29.81it/s]\u001b[A\n"," 44% 214/488 [00:07<00:09, 29.79it/s]\u001b[A\n"," 44% 217/488 [00:07<00:09, 29.80it/s]\u001b[A\n"," 45% 220/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 46% 223/488 [00:07<00:08, 29.82it/s]\u001b[A\n"," 46% 226/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 47% 229/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 48% 232/488 [00:07<00:08, 29.78it/s]\u001b[A\n"," 48% 235/488 [00:07<00:08, 29.75it/s]\u001b[A\n"," 49% 238/488 [00:07<00:08, 29.67it/s]\u001b[A\n"," 49% 241/488 [00:08<00:08, 29.71it/s]\u001b[A\n"," 50% 244/488 [00:08<00:08, 29.74it/s]\u001b[A\n"," 51% 247/488 [00:08<00:08, 29.74it/s]\u001b[A\n"," 51% 250/488 [00:08<00:07, 29.76it/s]\u001b[A\n"," 52% 253/488 [00:08<00:07, 29.78it/s]\u001b[A\n"," 52% 256/488 [00:08<00:07, 29.78it/s]\u001b[A\n"," 53% 259/488 [00:08<00:07, 29.79it/s]\u001b[A\n"," 54% 262/488 [00:08<00:07, 29.80it/s]\u001b[A\n"," 54% 265/488 [00:08<00:07, 29.78it/s]\u001b[A\n"," 55% 268/488 [00:08<00:07, 29.79it/s]\u001b[A\n"," 56% 271/488 [00:09<00:07, 29.79it/s]\u001b[A\n"," 56% 274/488 [00:09<00:07, 29.73it/s]\u001b[A\n"," 57% 277/488 [00:09<00:07, 29.75it/s]\u001b[A\n"," 57% 280/488 [00:09<00:06, 29.74it/s]\u001b[A\n"," 58% 283/488 [00:09<00:06, 29.75it/s]\u001b[A\n"," 59% 286/488 [00:09<00:06, 29.76it/s]\u001b[A\n"," 59% 289/488 [00:09<00:06, 29.78it/s]\u001b[A\n"," 60% 292/488 [00:09<00:06, 29.79it/s]\u001b[A\n"," 60% 295/488 [00:09<00:06, 29.78it/s]\u001b[A\n"," 61% 298/488 [00:09<00:06, 29.80it/s]\u001b[A\n"," 62% 301/488 [00:10<00:06, 29.80it/s]\u001b[A\n"," 62% 304/488 [00:10<00:06, 29.79it/s]\u001b[A\n"," 63% 307/488 [00:10<00:06, 29.79it/s]\u001b[A\n"," 64% 310/488 [00:10<00:05, 29.78it/s]\u001b[A\n"," 64% 313/488 [00:10<00:05, 29.75it/s]\u001b[A\n"," 65% 316/488 [00:10<00:05, 29.76it/s]\u001b[A\n"," 65% 319/488 [00:10<00:05, 29.75it/s]\u001b[A\n"," 66% 322/488 [00:10<00:05, 29.74it/s]\u001b[A\n"," 67% 325/488 [00:10<00:05, 29.75it/s]\u001b[A\n"," 67% 328/488 [00:11<00:05, 29.78it/s]\u001b[A\n"," 68% 331/488 [00:11<00:05, 29.79it/s]\u001b[A\n"," 68% 334/488 [00:11<00:05, 29.80it/s]\u001b[A\n"," 69% 337/488 [00:11<00:05, 29.81it/s]\u001b[A\n"," 70% 340/488 [00:11<00:04, 29.81it/s]\u001b[A\n"," 70% 343/488 [00:11<00:04, 29.81it/s]\u001b[A\n"," 71% 346/488 [00:11<00:04, 29.81it/s]\u001b[A\n"," 72% 349/488 [00:11<00:04, 29.81it/s]\u001b[A\n"," 72% 352/488 [00:11<00:04, 29.80it/s]\u001b[A\n"," 73% 355/488 [00:11<00:04, 29.80it/s]\u001b[A\n"," 73% 358/488 [00:12<00:04, 29.80it/s]\u001b[A\n"," 74% 361/488 [00:12<00:04, 29.78it/s]\u001b[A\n"," 75% 364/488 [00:12<00:04, 29.77it/s]\u001b[A\n"," 75% 367/488 [00:12<00:04, 29.79it/s]\u001b[A\n"," 76% 370/488 [00:12<00:03, 29.78it/s]\u001b[A\n"," 76% 373/488 [00:12<00:03, 29.76it/s]\u001b[A\n"," 77% 376/488 [00:12<00:03, 29.77it/s]\u001b[A\n"," 78% 379/488 [00:12<00:03, 29.76it/s]\u001b[A\n"," 78% 382/488 [00:12<00:03, 29.76it/s]\u001b[A\n"," 79% 385/488 [00:12<00:03, 29.77it/s]\u001b[A\n"," 80% 388/488 [00:13<00:03, 29.78it/s]\u001b[A\n"," 80% 391/488 [00:13<00:03, 29.79it/s]\u001b[A\n"," 81% 394/488 [00:13<00:03, 29.52it/s]\u001b[A\n"," 81% 397/488 [00:13<00:03, 29.60it/s]\u001b[A\n"," 82% 400/488 [00:13<00:02, 29.65it/s]\u001b[A\n"," 83% 403/488 [00:13<00:02, 29.69it/s]\u001b[A\n"," 83% 406/488 [00:13<00:02, 29.73it/s]\u001b[A\n"," 84% 409/488 [00:13<00:02, 29.36it/s]\u001b[A\n"," 84% 412/488 [00:13<00:02, 29.48it/s]\u001b[A\n"," 85% 415/488 [00:13<00:02, 29.57it/s]\u001b[A\n"," 86% 418/488 [00:14<00:02, 29.61it/s]\u001b[A\n"," 86% 421/488 [00:14<00:02, 29.65it/s]\u001b[A\n"," 87% 424/488 [00:14<00:02, 29.71it/s]\u001b[A\n"," 88% 427/488 [00:14<00:02, 29.71it/s]\u001b[A\n"," 88% 430/488 [00:14<00:01, 29.71it/s]\u001b[A\n"," 89% 433/488 [00:14<00:01, 29.74it/s]\u001b[A\n"," 89% 436/488 [00:14<00:01, 29.34it/s]\u001b[A\n"," 90% 439/488 [00:14<00:01, 29.48it/s]\u001b[A\n"," 91% 442/488 [00:14<00:01, 29.57it/s]\u001b[A\n"," 91% 445/488 [00:14<00:01, 29.64it/s]\u001b[A\n"," 92% 448/488 [00:15<00:01, 29.70it/s]\u001b[A\n"," 92% 451/488 [00:15<00:01, 29.51it/s]\u001b[A\n"," 93% 454/488 [00:15<00:01, 29.55it/s]\u001b[A\n"," 94% 457/488 [00:15<00:01, 29.59it/s]\u001b[A\n"," 94% 460/488 [00:15<00:00, 29.62it/s]\u001b[A\n"," 95% 463/488 [00:15<00:00, 29.60it/s]\u001b[A\n"," 95% 466/488 [00:15<00:00, 29.61it/s]\u001b[A\n"," 96% 469/488 [00:15<00:00, 29.63it/s]\u001b[A\n"," 97% 472/488 [00:15<00:00, 29.64it/s]\u001b[A\n"," 97% 475/488 [00:15<00:00, 29.65it/s]\u001b[A\n"," 98% 478/488 [00:16<00:00, 29.64it/s]\u001b[A\n"," 99% 481/488 [00:16<00:00, 29.65it/s]\u001b[A\n"," 99% 484/488 [00:16<00:00, 29.67it/s]\u001b[A\n","                                            \n","\u001b[A{'eval_loss': 1.0251054763793945, 'eval_macro-f1': 0.70069356701423, 'eval_micro-f1': 0.7007692307692308, 'eval_runtime': 16.4258, 'eval_samples_per_second': 237.432, 'eval_steps_per_second': 29.709, 'epoch': 3.0}\n"," 15% 16875/112500 [31:44<2:53:27,  9.19it/s]\n","100% 488/488 [00:16<00:00, 29.67it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:2916] 2023-06-09 00:14:43,773 >> Saving model checkpoint to logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-16875\n","[INFO|configuration_utils.py:458] 2023-06-09 00:14:43,774 >> Configuration saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-16875/config.json\n","[INFO|modeling_utils.py:1853] 2023-06-09 00:14:44,328 >> Model weights saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-16875/pytorch_model.bin\n","{'loss': 0.3888, 'learning_rate': 2.5469333333333337e-05, 'epoch': 3.02}\n","{'loss': 0.2526, 'learning_rate': 2.5336000000000003e-05, 'epoch': 3.11}\n","{'loss': 0.2515, 'learning_rate': 2.520266666666667e-05, 'epoch': 3.2}\n","{'loss': 0.2843, 'learning_rate': 2.50696e-05, 'epoch': 3.29}\n","{'loss': 0.2698, 'learning_rate': 2.4936533333333334e-05, 'epoch': 3.38}\n","{'loss': 0.2784, 'learning_rate': 2.48032e-05, 'epoch': 3.47}\n","{'loss': 0.3147, 'learning_rate': 2.4669866666666666e-05, 'epoch': 3.56}\n","{'loss': 0.3195, 'learning_rate': 2.4536533333333335e-05, 'epoch': 3.64}\n","{'loss': 0.3193, 'learning_rate': 2.4403200000000004e-05, 'epoch': 3.73}\n","{'loss': 0.3119, 'learning_rate': 2.426986666666667e-05, 'epoch': 3.82}\n","{'loss': 0.3443, 'learning_rate': 2.4136800000000002e-05, 'epoch': 3.91}\n","{'loss': 0.3333, 'learning_rate': 2.4003466666666668e-05, 'epoch': 4.0}\n"," 20% 22500/112500 [41:59<2:42:49,  9.21it/s][INFO|trainer.py:3190] 2023-06-09 00:24:59,355 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3192] 2023-06-09 00:24:59,356 >>   Num examples = 3900\n","[INFO|trainer.py:3195] 2023-06-09 00:24:59,356 >>   Batch size = 8\n","\n","  0% 0/488 [00:00<?, ?it/s]\u001b[A\n","  1% 4/488 [00:00<00:12, 39.26it/s]\u001b[A\n","  2% 8/488 [00:00<00:14, 33.12it/s]\u001b[A\n","  2% 12/488 [00:00<00:15, 31.53it/s]\u001b[A\n","  3% 16/488 [00:00<00:15, 30.85it/s]\u001b[A\n","  4% 20/488 [00:00<00:15, 30.48it/s]\u001b[A\n","  5% 24/488 [00:00<00:15, 30.26it/s]\u001b[A\n","  6% 28/488 [00:00<00:15, 30.11it/s]\u001b[A\n","  7% 32/488 [00:01<00:15, 30.02it/s]\u001b[A\n","  7% 36/488 [00:01<00:15, 29.92it/s]\u001b[A\n","  8% 39/488 [00:01<00:15, 29.89it/s]\u001b[A\n","  9% 42/488 [00:01<00:14, 29.87it/s]\u001b[A\n","  9% 45/488 [00:01<00:14, 29.86it/s]\u001b[A\n"," 10% 48/488 [00:01<00:14, 29.85it/s]\u001b[A\n"," 10% 51/488 [00:01<00:14, 29.76it/s]\u001b[A\n"," 11% 54/488 [00:01<00:14, 29.78it/s]\u001b[A\n"," 12% 57/488 [00:01<00:14, 29.79it/s]\u001b[A\n"," 12% 60/488 [00:01<00:14, 29.80it/s]\u001b[A\n"," 13% 63/488 [00:02<00:14, 29.78it/s]\u001b[A\n"," 14% 66/488 [00:02<00:14, 29.79it/s]\u001b[A\n"," 14% 69/488 [00:02<00:14, 29.47it/s]\u001b[A\n"," 15% 72/488 [00:02<00:14, 29.58it/s]\u001b[A\n"," 15% 75/488 [00:02<00:14, 29.45it/s]\u001b[A\n"," 16% 78/488 [00:02<00:13, 29.57it/s]\u001b[A\n"," 17% 81/488 [00:02<00:13, 29.59it/s]\u001b[A\n"," 17% 84/488 [00:02<00:13, 29.64it/s]\u001b[A\n"," 18% 87/488 [00:02<00:13, 29.70it/s]\u001b[A\n"," 18% 90/488 [00:02<00:13, 29.75it/s]\u001b[A\n"," 19% 93/488 [00:03<00:13, 29.78it/s]\u001b[A\n"," 20% 96/488 [00:03<00:13, 29.79it/s]\u001b[A\n"," 20% 99/488 [00:03<00:13, 29.80it/s]\u001b[A\n"," 21% 102/488 [00:03<00:12, 29.78it/s]\u001b[A\n"," 22% 105/488 [00:03<00:12, 29.80it/s]\u001b[A\n"," 22% 108/488 [00:03<00:12, 29.82it/s]\u001b[A\n"," 23% 111/488 [00:03<00:12, 29.83it/s]\u001b[A\n"," 23% 114/488 [00:03<00:12, 29.77it/s]\u001b[A\n"," 24% 117/488 [00:03<00:12, 29.80it/s]\u001b[A\n"," 25% 120/488 [00:03<00:12, 29.82it/s]\u001b[A\n"," 25% 123/488 [00:04<00:12, 29.81it/s]\u001b[A\n"," 26% 126/488 [00:04<00:12, 29.81it/s]\u001b[A\n"," 26% 129/488 [00:04<00:12, 29.80it/s]\u001b[A\n"," 27% 132/488 [00:04<00:11, 29.79it/s]\u001b[A\n"," 28% 135/488 [00:04<00:11, 29.81it/s]\u001b[A\n"," 28% 138/488 [00:04<00:11, 29.82it/s]\u001b[A\n"," 29% 141/488 [00:04<00:11, 29.78it/s]\u001b[A\n"," 30% 144/488 [00:04<00:11, 29.79it/s]\u001b[A\n"," 30% 147/488 [00:04<00:11, 29.79it/s]\u001b[A\n"," 31% 150/488 [00:05<00:11, 29.80it/s]\u001b[A\n"," 31% 153/488 [00:05<00:11, 29.80it/s]\u001b[A\n"," 32% 156/488 [00:05<00:11, 29.80it/s]\u001b[A\n"," 33% 159/488 [00:05<00:11, 29.75it/s]\u001b[A\n"," 33% 162/488 [00:05<00:10, 29.77it/s]\u001b[A\n"," 34% 165/488 [00:05<00:10, 29.78it/s]\u001b[A\n"," 34% 168/488 [00:05<00:10, 29.80it/s]\u001b[A\n"," 35% 171/488 [00:05<00:10, 29.79it/s]\u001b[A\n"," 36% 174/488 [00:05<00:10, 29.80it/s]\u001b[A\n"," 36% 177/488 [00:05<00:10, 29.80it/s]\u001b[A\n"," 37% 180/488 [00:06<00:10, 29.80it/s]\u001b[A\n"," 38% 183/488 [00:06<00:10, 29.82it/s]\u001b[A\n"," 38% 186/488 [00:06<00:10, 29.83it/s]\u001b[A\n"," 39% 189/488 [00:06<00:10, 29.84it/s]\u001b[A\n"," 39% 192/488 [00:06<00:09, 29.82it/s]\u001b[A\n"," 40% 195/488 [00:06<00:09, 29.84it/s]\u001b[A\n"," 41% 198/488 [00:06<00:09, 29.85it/s]\u001b[A\n"," 41% 201/488 [00:06<00:09, 29.85it/s]\u001b[A\n"," 42% 204/488 [00:06<00:09, 29.84it/s]\u001b[A\n"," 42% 207/488 [00:06<00:09, 29.84it/s]\u001b[A\n"," 43% 210/488 [00:07<00:09, 29.84it/s]\u001b[A\n"," 44% 213/488 [00:07<00:09, 29.82it/s]\u001b[A\n"," 44% 216/488 [00:07<00:09, 29.84it/s]\u001b[A\n"," 45% 219/488 [00:07<00:09, 29.80it/s]\u001b[A\n"," 45% 222/488 [00:07<00:08, 29.79it/s]\u001b[A\n"," 46% 225/488 [00:07<00:08, 29.79it/s]\u001b[A\n"," 47% 228/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 47% 231/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 48% 234/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 49% 237/488 [00:07<00:08, 29.75it/s]\u001b[A\n"," 49% 240/488 [00:08<00:08, 29.78it/s]\u001b[A\n"," 50% 243/488 [00:08<00:08, 29.80it/s]\u001b[A\n"," 50% 246/488 [00:08<00:08, 29.80it/s]\u001b[A\n"," 51% 249/488 [00:08<00:08, 29.80it/s]\u001b[A\n"," 52% 252/488 [00:08<00:07, 29.80it/s]\u001b[A\n"," 52% 255/488 [00:08<00:07, 29.82it/s]\u001b[A\n"," 53% 258/488 [00:08<00:07, 29.80it/s]\u001b[A\n"," 53% 261/488 [00:08<00:07, 29.79it/s]\u001b[A\n"," 54% 264/488 [00:08<00:07, 29.79it/s]\u001b[A\n"," 55% 267/488 [00:08<00:07, 29.79it/s]\u001b[A\n"," 55% 270/488 [00:09<00:07, 29.76it/s]\u001b[A\n"," 56% 273/488 [00:09<00:07, 29.78it/s]\u001b[A\n"," 57% 276/488 [00:09<00:07, 29.78it/s]\u001b[A\n"," 57% 279/488 [00:09<00:07, 29.78it/s]\u001b[A\n"," 58% 282/488 [00:09<00:06, 29.76it/s]\u001b[A\n"," 58% 285/488 [00:09<00:06, 29.55it/s]\u001b[A\n"," 59% 288/488 [00:09<00:06, 29.55it/s]\u001b[A\n"," 60% 291/488 [00:09<00:06, 29.61it/s]\u001b[A\n"," 60% 294/488 [00:09<00:06, 29.63it/s]\u001b[A\n"," 61% 297/488 [00:09<00:06, 29.64it/s]\u001b[A\n"," 61% 300/488 [00:10<00:06, 29.67it/s]\u001b[A\n"," 62% 303/488 [00:10<00:06, 29.65it/s]\u001b[A\n"," 63% 306/488 [00:10<00:06, 29.67it/s]\u001b[A\n"," 63% 309/488 [00:10<00:06, 29.68it/s]\u001b[A\n"," 64% 312/488 [00:10<00:05, 29.72it/s]\u001b[A\n"," 65% 315/488 [00:10<00:05, 29.73it/s]\u001b[A\n"," 65% 318/488 [00:10<00:05, 29.72it/s]\u001b[A\n"," 66% 321/488 [00:10<00:05, 29.72it/s]\u001b[A\n"," 66% 324/488 [00:10<00:05, 29.69it/s]\u001b[A\n"," 67% 327/488 [00:10<00:05, 29.31it/s]\u001b[A\n"," 68% 330/488 [00:11<00:05, 29.46it/s]\u001b[A\n"," 68% 333/488 [00:11<00:05, 29.56it/s]\u001b[A\n"," 69% 336/488 [00:11<00:05, 29.63it/s]\u001b[A\n"," 69% 339/488 [00:11<00:05, 29.14it/s]\u001b[A\n"," 70% 342/488 [00:11<00:04, 29.34it/s]\u001b[A\n"," 71% 345/488 [00:11<00:04, 29.37it/s]\u001b[A\n"," 71% 348/488 [00:11<00:04, 29.51it/s]\u001b[A\n"," 72% 351/488 [00:11<00:04, 29.61it/s]\u001b[A\n"," 73% 354/488 [00:11<00:04, 29.68it/s]\u001b[A\n"," 73% 357/488 [00:11<00:04, 29.73it/s]\u001b[A\n"," 74% 360/488 [00:12<00:04, 29.77it/s]\u001b[A\n"," 74% 363/488 [00:12<00:04, 29.80it/s]\u001b[A\n"," 75% 366/488 [00:12<00:04, 29.79it/s]\u001b[A\n"," 76% 369/488 [00:12<00:03, 29.81it/s]\u001b[A\n"," 76% 372/488 [00:12<00:03, 29.81it/s]\u001b[A\n"," 77% 375/488 [00:12<00:03, 29.83it/s]\u001b[A\n"," 77% 378/488 [00:12<00:03, 29.83it/s]\u001b[A\n"," 78% 381/488 [00:12<00:03, 29.83it/s]\u001b[A\n"," 79% 384/488 [00:12<00:03, 29.83it/s]\u001b[A\n"," 79% 387/488 [00:12<00:03, 29.79it/s]\u001b[A\n"," 80% 390/488 [00:13<00:03, 29.81it/s]\u001b[A\n"," 81% 393/488 [00:13<00:03, 29.82it/s]\u001b[A\n"," 81% 396/488 [00:13<00:03, 29.81it/s]\u001b[A\n"," 82% 399/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 82% 402/488 [00:13<00:02, 29.81it/s]\u001b[A\n"," 83% 405/488 [00:13<00:02, 29.82it/s]\u001b[A\n"," 84% 408/488 [00:13<00:02, 29.83it/s]\u001b[A\n"," 84% 411/488 [00:13<00:02, 29.84it/s]\u001b[A\n"," 85% 414/488 [00:13<00:02, 29.83it/s]\u001b[A\n"," 85% 417/488 [00:13<00:02, 29.82it/s]\u001b[A\n"," 86% 420/488 [00:14<00:02, 29.83it/s]\u001b[A\n"," 87% 423/488 [00:14<00:02, 29.83it/s]\u001b[A\n"," 87% 426/488 [00:14<00:02, 29.78it/s]\u001b[A\n"," 88% 429/488 [00:14<00:01, 29.77it/s]\u001b[A\n"," 89% 432/488 [00:14<00:01, 29.71it/s]\u001b[A\n"," 89% 435/488 [00:14<00:01, 29.74it/s]\u001b[A\n"," 90% 438/488 [00:14<00:01, 29.75it/s]\u001b[A\n"," 90% 441/488 [00:14<00:01, 29.77it/s]\u001b[A\n"," 91% 444/488 [00:14<00:01, 29.78it/s]\u001b[A\n"," 92% 447/488 [00:14<00:01, 29.78it/s]\u001b[A\n"," 92% 450/488 [00:15<00:01, 29.79it/s]\u001b[A\n"," 93% 453/488 [00:15<00:01, 29.80it/s]\u001b[A\n"," 93% 456/488 [00:15<00:01, 29.78it/s]\u001b[A\n"," 94% 459/488 [00:15<00:00, 29.80it/s]\u001b[A\n"," 95% 462/488 [00:15<00:00, 29.80it/s]\u001b[A\n"," 95% 465/488 [00:15<00:00, 29.81it/s]\u001b[A\n"," 96% 468/488 [00:15<00:00, 29.81it/s]\u001b[A\n"," 97% 471/488 [00:15<00:00, 29.81it/s]\u001b[A\n"," 97% 474/488 [00:15<00:00, 29.81it/s]\u001b[A\n"," 98% 477/488 [00:15<00:00, 29.81it/s]\u001b[A\n"," 98% 480/488 [00:16<00:00, 29.82it/s]\u001b[A\n"," 99% 483/488 [00:16<00:00, 29.83it/s]\u001b[A\n","                                            \n","\u001b[A{'eval_loss': 1.382757544517517, 'eval_macro-f1': 0.6967742550027411, 'eval_micro-f1': 0.696923076923077, 'eval_runtime': 16.3941, 'eval_samples_per_second': 237.89, 'eval_steps_per_second': 29.767, 'epoch': 4.0}\n"," 20% 22500/112500 [42:16<2:42:49,  9.21it/s]\n","100% 488/488 [00:16<00:00, 29.81it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:2916] 2023-06-09 00:25:15,751 >> Saving model checkpoint to logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-22500\n","[INFO|configuration_utils.py:458] 2023-06-09 00:25:15,752 >> Configuration saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1853] 2023-06-09 00:25:16,300 >> Model weights saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-22500/pytorch_model.bin\n","{'loss': 0.1849, 'learning_rate': 2.3870133333333334e-05, 'epoch': 4.09}\n","{'loss': 0.1656, 'learning_rate': 2.37368e-05, 'epoch': 4.18}\n","{'loss': 0.1641, 'learning_rate': 2.360346666666667e-05, 'epoch': 4.27}\n","{'loss': 0.2205, 'learning_rate': 2.3470133333333334e-05, 'epoch': 4.36}\n","{'loss': 0.199, 'learning_rate': 2.3337066666666667e-05, 'epoch': 4.44}\n","{'loss': 0.2206, 'learning_rate': 2.3203733333333336e-05, 'epoch': 4.53}\n","{'loss': 0.2036, 'learning_rate': 2.3070400000000002e-05, 'epoch': 4.62}\n","{'loss': 0.2338, 'learning_rate': 2.2937066666666668e-05, 'epoch': 4.71}\n","{'loss': 0.2511, 'learning_rate': 2.2804266666666668e-05, 'epoch': 4.8}\n","{'loss': 0.2469, 'learning_rate': 2.2670933333333333e-05, 'epoch': 4.89}\n","{'loss': 0.2768, 'learning_rate': 2.25376e-05, 'epoch': 4.98}\n"," 25% 28125/112500 [52:31<2:34:21,  9.11it/s][INFO|trainer.py:3190] 2023-06-09 00:35:31,271 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3192] 2023-06-09 00:35:31,272 >>   Num examples = 3900\n","[INFO|trainer.py:3195] 2023-06-09 00:35:31,272 >>   Batch size = 8\n","\n","  0% 0/488 [00:00<?, ?it/s]\u001b[A\n","  1% 4/488 [00:00<00:12, 39.76it/s]\u001b[A\n","  2% 8/488 [00:00<00:14, 33.26it/s]\u001b[A\n","  2% 12/488 [00:00<00:15, 31.58it/s]\u001b[A\n","  3% 16/488 [00:00<00:15, 30.86it/s]\u001b[A\n","  4% 20/488 [00:00<00:15, 30.50it/s]\u001b[A\n","  5% 24/488 [00:00<00:15, 30.22it/s]\u001b[A\n","  6% 28/488 [00:00<00:15, 30.08it/s]\u001b[A\n","  7% 32/488 [00:01<00:15, 30.00it/s]\u001b[A\n","  7% 36/488 [00:01<00:15, 29.95it/s]\u001b[A\n","  8% 39/488 [00:01<00:15, 29.91it/s]\u001b[A\n","  9% 42/488 [00:01<00:14, 29.89it/s]\u001b[A\n","  9% 45/488 [00:01<00:14, 29.87it/s]\u001b[A\n"," 10% 48/488 [00:01<00:14, 29.81it/s]\u001b[A\n"," 10% 51/488 [00:01<00:14, 29.81it/s]\u001b[A\n"," 11% 54/488 [00:01<00:14, 29.79it/s]\u001b[A\n"," 12% 57/488 [00:01<00:14, 29.77it/s]\u001b[A\n"," 12% 60/488 [00:01<00:14, 29.74it/s]\u001b[A\n"," 13% 63/488 [00:02<00:14, 29.77it/s]\u001b[A\n"," 14% 66/488 [00:02<00:14, 29.73it/s]\u001b[A\n"," 14% 69/488 [00:02<00:14, 29.76it/s]\u001b[A\n"," 15% 72/488 [00:02<00:13, 29.77it/s]\u001b[A\n"," 15% 75/488 [00:02<00:13, 29.78it/s]\u001b[A\n"," 16% 78/488 [00:02<00:13, 29.79it/s]\u001b[A\n"," 17% 81/488 [00:02<00:13, 29.68it/s]\u001b[A\n"," 17% 84/488 [00:02<00:13, 29.70it/s]\u001b[A\n"," 18% 87/488 [00:02<00:13, 29.71it/s]\u001b[A\n"," 18% 90/488 [00:02<00:13, 29.58it/s]\u001b[A\n"," 19% 93/488 [00:03<00:13, 29.63it/s]\u001b[A\n"," 20% 96/488 [00:03<00:13, 29.69it/s]\u001b[A\n"," 20% 99/488 [00:03<00:13, 29.75it/s]\u001b[A\n"," 21% 102/488 [00:03<00:12, 29.79it/s]\u001b[A\n"," 22% 105/488 [00:03<00:12, 29.82it/s]\u001b[A\n"," 22% 108/488 [00:03<00:12, 29.85it/s]\u001b[A\n"," 23% 111/488 [00:03<00:12, 29.86it/s]\u001b[A\n"," 23% 114/488 [00:03<00:12, 29.84it/s]\u001b[A\n"," 24% 117/488 [00:03<00:12, 29.84it/s]\u001b[A\n"," 25% 120/488 [00:03<00:12, 29.84it/s]\u001b[A\n"," 25% 123/488 [00:04<00:12, 29.80it/s]\u001b[A\n"," 26% 126/488 [00:04<00:12, 29.82it/s]\u001b[A\n"," 26% 129/488 [00:04<00:12, 29.83it/s]\u001b[A\n"," 27% 132/488 [00:04<00:11, 29.82it/s]\u001b[A\n"," 28% 135/488 [00:04<00:11, 29.77it/s]\u001b[A\n"," 28% 138/488 [00:04<00:11, 29.79it/s]\u001b[A\n"," 29% 141/488 [00:04<00:11, 29.81it/s]\u001b[A\n"," 30% 144/488 [00:04<00:11, 29.82it/s]\u001b[A\n"," 30% 147/488 [00:04<00:11, 29.82it/s]\u001b[A\n"," 31% 150/488 [00:05<00:11, 29.83it/s]\u001b[A\n"," 31% 153/488 [00:05<00:11, 29.82it/s]\u001b[A\n"," 32% 156/488 [00:05<00:11, 29.83it/s]\u001b[A\n"," 33% 159/488 [00:05<00:11, 29.84it/s]\u001b[A\n"," 33% 162/488 [00:05<00:10, 29.84it/s]\u001b[A\n"," 34% 165/488 [00:05<00:10, 29.85it/s]\u001b[A\n"," 34% 168/488 [00:05<00:10, 29.85it/s]\u001b[A\n"," 35% 171/488 [00:05<00:10, 29.86it/s]\u001b[A\n"," 36% 174/488 [00:05<00:10, 29.82it/s]\u001b[A\n"," 36% 177/488 [00:05<00:10, 29.82it/s]\u001b[A\n"," 37% 180/488 [00:06<00:10, 29.81it/s]\u001b[A\n"," 38% 183/488 [00:06<00:10, 29.82it/s]\u001b[A\n"," 38% 186/488 [00:06<00:10, 29.82it/s]\u001b[A\n"," 39% 189/488 [00:06<00:10, 29.84it/s]\u001b[A\n"," 39% 192/488 [00:06<00:09, 29.83it/s]\u001b[A\n"," 40% 195/488 [00:06<00:09, 29.84it/s]\u001b[A\n"," 41% 198/488 [00:06<00:09, 29.85it/s]\u001b[A\n"," 41% 201/488 [00:06<00:09, 29.85it/s]\u001b[A\n"," 42% 204/488 [00:06<00:09, 29.80it/s]\u001b[A\n"," 42% 207/488 [00:06<00:09, 29.81it/s]\u001b[A\n"," 43% 210/488 [00:07<00:09, 29.82it/s]\u001b[A\n"," 44% 213/488 [00:07<00:09, 29.79it/s]\u001b[A\n"," 44% 216/488 [00:07<00:09, 29.78it/s]\u001b[A\n"," 45% 219/488 [00:07<00:09, 29.78it/s]\u001b[A\n"," 45% 222/488 [00:07<00:08, 29.81it/s]\u001b[A\n"," 46% 225/488 [00:07<00:08, 29.83it/s]\u001b[A\n"," 47% 228/488 [00:07<00:08, 29.83it/s]\u001b[A\n"," 47% 231/488 [00:07<00:08, 29.82it/s]\u001b[A\n"," 48% 234/488 [00:07<00:08, 29.84it/s]\u001b[A\n"," 49% 237/488 [00:07<00:08, 29.84it/s]\u001b[A\n"," 49% 240/488 [00:08<00:08, 29.84it/s]\u001b[A\n"," 50% 243/488 [00:08<00:08, 29.86it/s]\u001b[A\n"," 50% 246/488 [00:08<00:08, 29.85it/s]\u001b[A\n"," 51% 249/488 [00:08<00:08, 29.86it/s]\u001b[A\n"," 52% 252/488 [00:08<00:07, 29.84it/s]\u001b[A\n"," 52% 255/488 [00:08<00:07, 29.85it/s]\u001b[A\n"," 53% 258/488 [00:08<00:07, 29.85it/s]\u001b[A\n"," 53% 261/488 [00:08<00:07, 29.84it/s]\u001b[A\n"," 54% 264/488 [00:08<00:07, 29.81it/s]\u001b[A\n"," 55% 267/488 [00:08<00:07, 29.82it/s]\u001b[A\n"," 55% 270/488 [00:09<00:07, 29.79it/s]\u001b[A\n"," 56% 273/488 [00:09<00:07, 29.81it/s]\u001b[A\n"," 57% 276/488 [00:09<00:07, 29.82it/s]\u001b[A\n"," 57% 279/488 [00:09<00:07, 29.84it/s]\u001b[A\n"," 58% 282/488 [00:09<00:06, 29.86it/s]\u001b[A\n"," 58% 285/488 [00:09<00:06, 29.86it/s]\u001b[A\n"," 59% 288/488 [00:09<00:06, 29.86it/s]\u001b[A\n"," 60% 291/488 [00:09<00:06, 29.84it/s]\u001b[A\n"," 60% 294/488 [00:09<00:06, 29.83it/s]\u001b[A\n"," 61% 297/488 [00:09<00:06, 29.83it/s]\u001b[A\n"," 61% 300/488 [00:10<00:06, 29.81it/s]\u001b[A\n"," 62% 303/488 [00:10<00:06, 29.77it/s]\u001b[A\n"," 63% 306/488 [00:10<00:06, 29.79it/s]\u001b[A\n"," 63% 309/488 [00:10<00:06, 29.81it/s]\u001b[A\n"," 64% 312/488 [00:10<00:05, 29.83it/s]\u001b[A\n"," 65% 315/488 [00:10<00:05, 29.82it/s]\u001b[A\n"," 65% 318/488 [00:10<00:05, 29.82it/s]\u001b[A\n"," 66% 321/488 [00:10<00:05, 29.84it/s]\u001b[A\n"," 66% 324/488 [00:10<00:05, 29.84it/s]\u001b[A\n"," 67% 327/488 [00:10<00:05, 29.84it/s]\u001b[A\n"," 68% 330/488 [00:11<00:05, 29.84it/s]\u001b[A\n"," 68% 333/488 [00:11<00:05, 29.84it/s]\u001b[A\n"," 69% 336/488 [00:11<00:05, 29.85it/s]\u001b[A\n"," 69% 339/488 [00:11<00:04, 29.85it/s]\u001b[A\n"," 70% 342/488 [00:11<00:04, 29.84it/s]\u001b[A\n"," 71% 345/488 [00:11<00:04, 29.83it/s]\u001b[A\n"," 71% 348/488 [00:11<00:04, 29.78it/s]\u001b[A\n"," 72% 351/488 [00:11<00:04, 29.79it/s]\u001b[A\n"," 73% 354/488 [00:11<00:04, 29.81it/s]\u001b[A\n"," 73% 357/488 [00:11<00:04, 29.82it/s]\u001b[A\n"," 74% 360/488 [00:12<00:04, 29.80it/s]\u001b[A\n"," 74% 363/488 [00:12<00:04, 29.81it/s]\u001b[A\n"," 75% 366/488 [00:12<00:04, 29.82it/s]\u001b[A\n"," 76% 369/488 [00:12<00:03, 29.82it/s]\u001b[A\n"," 76% 372/488 [00:12<00:03, 29.81it/s]\u001b[A\n"," 77% 375/488 [00:12<00:03, 29.81it/s]\u001b[A\n"," 77% 378/488 [00:12<00:03, 29.82it/s]\u001b[A\n"," 78% 381/488 [00:12<00:03, 29.79it/s]\u001b[A\n"," 79% 384/488 [00:12<00:03, 29.81it/s]\u001b[A\n"," 79% 387/488 [00:12<00:03, 29.82it/s]\u001b[A\n"," 80% 390/488 [00:13<00:03, 29.81it/s]\u001b[A\n"," 81% 393/488 [00:13<00:03, 29.75it/s]\u001b[A\n"," 81% 396/488 [00:13<00:03, 29.75it/s]\u001b[A\n"," 82% 399/488 [00:13<00:02, 29.77it/s]\u001b[A\n"," 82% 402/488 [00:13<00:02, 29.79it/s]\u001b[A\n"," 83% 405/488 [00:13<00:02, 29.79it/s]\u001b[A\n"," 84% 408/488 [00:13<00:02, 29.79it/s]\u001b[A\n"," 84% 411/488 [00:13<00:02, 29.78it/s]\u001b[A\n"," 85% 414/488 [00:13<00:02, 29.78it/s]\u001b[A\n"," 85% 417/488 [00:13<00:02, 29.78it/s]\u001b[A\n"," 86% 420/488 [00:14<00:02, 29.77it/s]\u001b[A\n"," 87% 423/488 [00:14<00:02, 29.79it/s]\u001b[A\n"," 87% 426/488 [00:14<00:02, 29.81it/s]\u001b[A\n"," 88% 429/488 [00:14<00:01, 29.82it/s]\u001b[A\n"," 89% 432/488 [00:14<00:01, 29.81it/s]\u001b[A\n"," 89% 435/488 [00:14<00:01, 29.82it/s]\u001b[A\n"," 90% 438/488 [00:14<00:01, 29.82it/s]\u001b[A\n"," 90% 441/488 [00:14<00:01, 29.83it/s]\u001b[A\n"," 91% 444/488 [00:14<00:01, 29.84it/s]\u001b[A\n"," 92% 447/488 [00:14<00:01, 29.82it/s]\u001b[A\n"," 92% 450/488 [00:15<00:01, 29.78it/s]\u001b[A\n"," 93% 453/488 [00:15<00:01, 29.69it/s]\u001b[A\n"," 93% 456/488 [00:15<00:01, 29.72it/s]\u001b[A\n"," 94% 459/488 [00:15<00:00, 29.73it/s]\u001b[A\n"," 95% 462/488 [00:15<00:00, 29.74it/s]\u001b[A\n"," 95% 465/488 [00:15<00:00, 29.75it/s]\u001b[A\n"," 96% 468/488 [00:15<00:00, 29.75it/s]\u001b[A\n"," 97% 471/488 [00:15<00:00, 29.76it/s]\u001b[A\n"," 97% 474/488 [00:15<00:00, 29.76it/s]\u001b[A\n"," 98% 477/488 [00:15<00:00, 29.75it/s]\u001b[A\n"," 98% 480/488 [00:16<00:00, 29.75it/s]\u001b[A\n"," 99% 483/488 [00:16<00:00, 29.77it/s]\u001b[A\n","                                            \n","\u001b[A{'eval_loss': 1.9839166402816772, 'eval_macro-f1': 0.6838214916347278, 'eval_micro-f1': 0.6838461538461539, 'eval_runtime': 16.3677, 'eval_samples_per_second': 238.275, 'eval_steps_per_second': 29.815, 'epoch': 5.0}\n"," 25% 28125/112500 [52:48<2:34:21,  9.11it/s]\n","100% 488/488 [00:16<00:00, 29.79it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:2916] 2023-06-09 00:35:47,640 >> Saving model checkpoint to logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-28125\n","[INFO|configuration_utils.py:458] 2023-06-09 00:35:47,641 >> Configuration saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-28125/config.json\n","[INFO|modeling_utils.py:1853] 2023-06-09 00:35:48,194 >> Model weights saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-28125/pytorch_model.bin\n","[INFO|trainer.py:2044] 2023-06-09 00:35:49,338 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2179] 2023-06-09 00:35:49,339 >> Loading best model from logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/checkpoint-11250 (score: 0.7225641025641026).\n","{'train_runtime': 3170.0959, 'train_samples_per_second': 283.903, 'train_steps_per_second': 35.488, 'train_loss': 0.4708037590196398, 'epoch': 5.0}\n"," 25% 28125/112500 [52:50<2:38:30,  8.87it/s]\n","[INFO|trainer.py:2916] 2023-06-09 00:35:49,703 >> Saving model checkpoint to logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1\n","[INFO|configuration_utils.py:458] 2023-06-09 00:35:49,704 >> Configuration saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/config.json\n","[INFO|modeling_utils.py:1853] 2023-06-09 00:35:50,503 >> Model weights saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2194] 2023-06-09 00:35:50,504 >> tokenizer config file saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2201] 2023-06-09 00:35:50,504 >> Special tokens file saved in logs/case_hold//content/gdrive/MyDrive/saved_model/bert_full_more_data/seed_1/special_tokens_map.json\n","06/09/2023 00:35:50 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:3190] 2023-06-09 00:35:50,545 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3192] 2023-06-09 00:35:50,545 >>   Num examples = 3900\n","[INFO|trainer.py:3195] 2023-06-09 00:35:50,545 >>   Batch size = 8\n","100% 488/488 [00:15<00:00, 31.82it/s]\n","***** eval metrics *****\n","  epoch                   =        5.0\n","  eval_loss               =     0.7821\n","  eval_macro-f1           =     0.7224\n","  eval_micro-f1           =     0.7226\n","  eval_runtime            = 0:00:15.37\n","  eval_samples            =       3900\n","  eval_samples_per_second =    253.654\n","  eval_steps_per_second   =     31.739\n","06/09/2023 00:36:05 - INFO - __main__ -   *** Predict ***\n","[INFO|trainer.py:3190] 2023-06-09 00:36:05,920 >> ***** Running Prediction *****\n","[INFO|trainer.py:3192] 2023-06-09 00:36:05,920 >>   Num examples = 3600\n","[INFO|trainer.py:3195] 2023-06-09 00:36:05,920 >>   Batch size = 8\n","100% 450/450 [00:14<00:00, 31.80it/s]\n","***** predict metrics *****\n","  predict_loss               =     0.8644\n","  predict_macro-f1           =     0.6975\n","  predict_micro-f1           =     0.6975\n","  predict_runtime            = 0:00:14.18\n","  predict_samples            =       3600\n","  predict_samples_per_second =    253.832\n","  predict_steps_per_second   =     31.729\n","logs/case_hold exists!\n","                                     VALIDATION                                      | TEST\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","                  bert-base-uncased: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","                       roberta-base: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","             microsoft/deberta-base: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","       allenai/longformer-base-4096: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","        google/bigbird-roberta-base: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","    nlpaueb/legal-bert-base-uncased: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","            zlucia/custom-legalbert: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n","                      roberta-large: MICRO-F1: nan\t ± nan\tMACRO-F1: nan\t ± nan\t | MICRO-F1: nan\tMACRO-F1: nan\t\n"]}],"source":["!sh scripts/run_case_hold.sh"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vSxyJpV9Cvog","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686267352038,"user_tz":300,"elapsed":492,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"}},"outputId":"459accf7-a87f-4845-efed-4e4c3829540e"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'logs/scotus/': No such file or directory\n"]}],"source":["!rm -r logs/scotus/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1686192927200,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"DMUE7zX7vnK_","outputId":"f59b93c8-1931-4f7f-ccb1-a53bb8c91f57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved working directory and index state WIP on main: 1f4277a Deleted unwanted flags\n"]}],"source":["!git stash"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":632,"status":"ok","timestamp":1686192929572,"user":{"displayName":"Legal Chat","userId":"06499461168622735285"},"user_tz":300},"id":"ZTvSDswd5WjX","outputId":"067ce370-ef49-4b82-d93f-1dbe637f86c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Already up to date.\n"]}],"source":["!git pull"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1J7acrl3hO_tVVWGYzv55p6H2bapUa6OA","timestamp":1686189458845},{"file_id":"1sCMUfdcVJie0rfQutrcoGWjYJqBqvUG5","timestamp":1686182471979}],"gpuType":"A100","authorship_tag":"ABX9TyNgBnAZtOHxM4/2SqF0Wos6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}